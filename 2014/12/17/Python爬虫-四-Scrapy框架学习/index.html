<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Python爬虫(七)--Scrapy框架学习 · Snow Memory | Andrew Liu</title><meta name="description" content="Python爬虫(七)--Scrapy框架学习 - Andrew Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://andrewliu.in/atom.xml" title="Snow Memory | Andrew Liu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/dinosaurliu" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Andrew-liu" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Python爬虫(七)--Scrapy框架学习</h1><div class="post-info">Dec 17, 2014</div><div class="post-content"><p>本文希望达到以下目标:</p>
<ol>
<li>简要介绍Scarpy</li>
<li>阅读官网入门文档并实现文档中的范例</li>
<li>使用Scarpy优化豆瓣爬虫的抓取</li>
<li>制定下一步学习目标</li>
</ol>
<p>#1. Scrapy简介</p>
<blockquote>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试</p>
</blockquote>
<p><code>Scrapy</code> 使用了 <code>Twisted</code>异步网络库来处理网络通讯。整体架构大致如下</p>
<p><img src="http://newtonblogimg.qiniudn.com/Scrapy%20Architecture.png" alt="Scrapy"></p>
<a id="more"></a>
<p>Scrapy主要包括了以下组件：</p>
<ul>
<li>引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
<li>调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li>下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li>爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
<li>项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li>下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li>爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li>调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<p><strong>Scrapy运行流程大概如下：</strong></p>
<ol>
<li>首先，引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)</li>
<li>然后，爬虫解析Response</li>
<li>若是解析出实体（Item）,则交给实体管道进行进一步的处理。</li>
<li>若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取</li>
</ol>
<p>#2. 安装Scrapy</p>
<p>使用以下命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">sudo pip install virtualenv  #安装虚拟环境工具</div><div class="line">virtualenv ENV  #创建一个虚拟环境目录</div><div class="line">source ./ENV/bin/active  #激活虚拟环境</div><div class="line">pip install Scrapy</div><div class="line">#验证是否安装成功</div><div class="line">pip list</div><div class="line">#输出如下</div><div class="line">cffi (0.8.6)</div><div class="line">cryptography (0.6.1)</div><div class="line">cssselect (0.9.1)</div><div class="line">lxml (3.4.1)</div><div class="line">pip (1.5.6)</div><div class="line">pycparser (2.10)</div><div class="line">pyOpenSSL (0.14)</div><div class="line">queuelib (1.2.2)</div><div class="line">Scrapy (0.24.4)</div><div class="line">setuptools (3.6)</div><div class="line">six (1.8.0)</div><div class="line">Twisted (14.0.2)</div><div class="line">w3lib (1.10.0)</div><div class="line">wsgiref (0.1.2)</div><div class="line">zope.interface (4.1.1)</div></pre></td></tr></table></figure>
<p>#3. Scrapy Tutorial</p>
<p>在抓取之前, 你需要新建一个<code>Scrapy</code>工程. 进入一个你想用来保存代码的目录，然后执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy startproject tutorial</div></pre></td></tr></table></figure>
<p>这个命令会在当前目录下创建一个新目录 tutorial, 它的结构如下: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── scrapy.cfg</div><div class="line">└── tutorial</div><div class="line">    ├── __init__.py</div><div class="line">    ├── items.py</div><div class="line">    ├── pipelines.py</div><div class="line">    ├── settings.py</div><div class="line">    └── spiders</div><div class="line">        └── __init__.py</div></pre></td></tr></table></figure>
<p>这些文件主要是：</p>
<ul>
<li>scrapy.cfg: 项目配置文件</li>
<li>tutorial/: 项目python模块, 之后您将在此加入代码</li>
<li>tutorial/items.py: 项目items文件</li>
<li>tutorial/pipelines.py: 项目管道文件</li>
<li>tutorial/settings.py: 项目配置文件</li>
<li>tutorial/spiders: 放置spider的目录</li>
</ul>
<p>##3.1. 定义Item</p>
<p>Items是将要装载抓取的数据的容器，它工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误</p>
<p>通过创建<code>scrapy.Item类</code>, 并且定义类型为 <code>scrapy.Field</code> 的类属性来声明一个Item.<br>我们通过将需要的item模型化，来控制从 <code>dmoz.org</code> 获得的站点数据，比如我们要获得站点的名字，url 和网站描述，我们定义这三种属性的域。在 tutorial 目录下的 items.py 文件编辑</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from scrapy.item import Item, Field</div><div class="line"></div><div class="line"></div><div class="line">class DmozItem(Item):</div><div class="line">    # define the fields for your item here like:</div><div class="line">    name = Field()</div><div class="line">    description = Field()</div><div class="line">    url = Field()</div></pre></td></tr></table></figure>
<p>##3.2. 编写Spider</p>
<p>Spider 是用户编写的类, 用于从一个域（或域组）中抓取信息, 定义了用于下载的URL的初步列表, 如何跟踪链接，以及如何来解析这些网页的内容用于提取items。</p>
<p>要建立一个 Spider，继承 <code>scrapy.Spider</code> 基类，并确定三个主要的、强制的属性：</p>
<ul>
<li>name：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字.</li>
<li>start_urls：包含了Spider在启动时进行爬取的url列表。因此，第一个被获取到的页面将是其中之一。后续的URL则从初始的URL获取到的数据中提取。我们可以利用正则表达式定义和过滤需要进行跟进的链接。</li>
<li>parse()：是spider的一个方法。被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。<br>这个方法负责解析返回的数据、匹配抓取的数据(解析为 item )并跟踪更多的 URL。</li>
</ul>
<p>在 /tutorial/tutorial/spiders 目录下创建 <code>dmoz_spider.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        filename = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</div><div class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(response.body)</div></pre></td></tr></table></figure>
<p>##3.3. 爬取</p>
<p>当前项目结构<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">├── scrapy.cfg</div><div class="line">└── tutorial</div><div class="line">    ├── __init__.py</div><div class="line">    ├── items.py</div><div class="line">    ├── pipelines.py</div><div class="line">    ├── settings.py</div><div class="line">    └── spiders</div><div class="line">        ├── __init__.py</div><div class="line">        └── dmoz_spider.py</div></pre></td></tr></table></figure></p>
<p>到项目根目录, 然后运行命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy crawl dmoz</div></pre></td></tr></table></figure>
<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: tutorial)</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Optional features available: ssl, http11</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Overridden settings: &#123;&apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;], &apos;BOT_NAME&apos;: &apos;tutorial&apos;&#125;</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] INFO: Enabled item pipelines:</div><div class="line">2014-12-15 09:30:59+0800 [dmoz] INFO: Spider opened</div><div class="line">2014-12-15 09:30:59+0800 [dmoz] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023</div><div class="line">2014-12-15 09:30:59+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080</div><div class="line">2014-12-15 09:31:00+0800 [dmoz] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)</div><div class="line">2014-12-15 09:31:00+0800 [dmoz] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)</div><div class="line">2014-12-15 09:31:00+0800 [dmoz] INFO: Closing spider (finished)</div><div class="line">2014-12-15 09:31:00+0800 [dmoz] INFO: Dumping Scrapy stats:</div><div class="line">    &#123;&apos;downloader/request_bytes&apos;: 516,</div><div class="line">     &apos;downloader/request_count&apos;: 2,</div><div class="line">     &apos;downloader/request_method_count/GET&apos;: 2,</div><div class="line">     &apos;downloader/response_bytes&apos;: 16338,</div><div class="line">     &apos;downloader/response_count&apos;: 2,</div><div class="line">     &apos;downloader/response_status_count/200&apos;: 2,</div><div class="line">     &apos;finish_reason&apos;: &apos;finished&apos;,</div><div class="line">     &apos;finish_time&apos;: datetime.datetime(2014, 12, 15, 1, 31, 0, 666214),</div><div class="line">     &apos;log_count/DEBUG&apos;: 4,</div><div class="line">     &apos;log_count/INFO&apos;: 7,</div><div class="line">     &apos;response_received_count&apos;: 2,</div><div class="line">     &apos;scheduler/dequeued&apos;: 2,</div><div class="line">     &apos;scheduler/dequeued/memory&apos;: 2,</div><div class="line">     &apos;scheduler/enqueued&apos;: 2,</div><div class="line">     &apos;scheduler/enqueued/memory&apos;: 2,</div><div class="line">     &apos;start_time&apos;: datetime.datetime(2014, 12, 15, 1, 30, 59, 533207)&#125;</div><div class="line">2014-12-15 09:31:00+0800 [dmoz] INFO: Spider closed (finished)</div></pre></td></tr></table></figure>
<p>##3.4. 提取Items</p>
<p>###3.4.1. 介绍Selector</p>
<p>从网页中提取数据有很多方法。Scrapy使用了一种基于 <code>XPath</code> 或者 <code>CSS 表达式</code>机制： Scrapy Selectors </p>
<p>出XPath表达式的例子及对应的含义:</p>
<ul>
<li><code>/html/head/title</code>: 选择HTML文档中 <code>&lt;head&gt;</code> 标签内的 <code>&lt;title&gt;</code> 元素</li>
<li><code>/html/head/title/text()</code>: 选择 <code>&lt;title&gt;</code> 元素内的文本</li>
<li><code>//td</code>: 选择所有的 <code>&lt;td&gt;</code> 元素</li>
<li><code>//div[@class=&quot;mine&quot;]</code>: 选择所有具有<code>class=&quot;mine&quot;</code> 属性的 div 元素</li>
</ul>
<blockquote>
<p>等多强大的功能使用可以查看<a href="http://www.w3school.com.cn/xpath/" target="_blank" rel="external">XPath tutorial</a></p>
</blockquote>
<p>为了方便使用 XPaths，Scrapy 提供 Selector 类， 有四种方法 :</p>
<ul>
<li>xpath()：返回selectors列表, 每一个selector表示一个xpath参数表达式选择的节点.</li>
<li>css() : 返回selectors列表, 每一个selector表示CSS参数表达式选择的节点</li>
<li>extract()：返回一个unicode字符串，该字符串为XPath选择器返回的数据</li>
<li>re()： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来</li>
</ul>
<p>###3.4.2. 取出数据</p>
<p>首先使用谷歌浏览器开发者工具, 查看网站源码, 来看自己需要取出的数据形式(这种方法比较麻烦), 更简单的方法是直接对感兴趣的东西右键<code>审查元素</code>, 可以直接查看网站源码</p>
<p>在查看网站源码后, 网站信息在第二个<code>&lt;ul&gt;</code>内</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;ul class=&quot;directory-url&quot; style=&quot;margin-left:0;&quot;&gt;</div><div class="line">            </div><div class="line"> &lt;li&gt;&lt;a href=&quot;http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html&quot; class=&quot;listinglink&quot;&gt;Core Python Programming&lt;/a&gt; </div><div class="line">- By Wesley J. Chun; Prentice Hall PTR, 2001, ISBN 0130260363. For experienced developers to improve extant skills; professional level examples. Starts by introducing syntax, objects, error handling, functions, classes, built-ins. [Prentice Hall]</div><div class="line">&lt;div class=&quot;flag&quot;&gt;&lt;a href=&quot;/public/flag?cat=Computers%2FProgramming%2FLanguages%2FPython%2FBooks&amp;url=http%3A%2F%2Fwww.pearsonhighered.com%2Feducator%2Facademic%2Fproduct%2F0%2C%2C0130260363%2C00%252Ben-USS_01DBC.html&quot;&gt;&lt;img src=&quot;/img/flag.png&quot; alt=&quot;[!]&quot; title=&quot;report an issue with this listing&quot;&gt;&lt;/a&gt;&lt;/div&gt;</div><div class="line">&lt;/li&gt;</div><div class="line">...省略部分...</div><div class="line">&lt;/ul&gt;</div></pre></td></tr></table></figure>
<p>那么就可以通过一下方式进行提取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#通过如下命令选择每个在网站中的 &lt;li&gt; 元素:</span></div><div class="line">sel.xpath(<span class="string">'//ul/li'</span>) </div><div class="line"></div><div class="line"><span class="comment">#网站描述:</span></div><div class="line">sel.xpath(<span class="string">'//ul/li/text()'</span>).extract()</div><div class="line"></div><div class="line"><span class="comment">#网站标题:</span></div><div class="line">sel.xpath(<span class="string">'//ul/li/a/text()'</span>).extract()</div><div class="line"></div><div class="line"><span class="comment">#网站链接:</span></div><div class="line">sel.xpath(<span class="string">'//ul/li/a/@href'</span>).extract()</div><div class="line"></div><div class="line"><span class="comment">#如前所述，每个 xpath() 调用返回一个 selectors 列表，所以我们可以结合 xpath() 去挖掘更深的节点。我们将会用到这些特性，所以:</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>)</div><div class="line">    title = sel.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">    link = sel.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">    desc = sel.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">    <span class="keyword">print</span> title, link, desc</div></pre></td></tr></table></figure>
<p>在已经的爬虫文件中修改代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class DmozSpider(scrapy.Spider):</div><div class="line">    name = &quot;dmoz&quot;</div><div class="line">    allowed_domains = [&quot;dmoz.org&quot;]</div><div class="line">    start_urls = [</div><div class="line">        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,</div><div class="line">        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</div><div class="line">    ]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        for sel in response.xpath(&apos;//ul/li&apos;):</div><div class="line">            title = sel.xpath(&apos;a/text()&apos;).extract()</div><div class="line">            link = sel.xpath(&apos;a/@href&apos;).extract()</div><div class="line">            desc = sel.xpath(&apos;text()&apos;).extract()</div><div class="line">            print title, link, desc</div></pre></td></tr></table></figure>
<p>###3.4.3. 使用item</p>
<p><code>Item</code>对象是自定义的python字典,可以使用标准的字典语法来获取到其每个字段的值(字段即是我们之前用Field赋值的属性)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; item = DmozItem()</div><div class="line">&gt;&gt;&gt; item[&apos;title&apos;] = &apos;Example title&apos;</div><div class="line">&gt;&gt;&gt; item[&apos;title&apos;]</div><div class="line">&apos;Example title&apos;</div></pre></td></tr></table></figure>
<p>一般来说，Spider将会将爬取到的数据以 Item 对象返回, 最后修改爬虫类，使用 Item 来保存数据，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.spider <span class="keyword">import</span> Spider</div><div class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</div><div class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem </div><div class="line"></div><div class="line">            </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span>,</div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        sel = Selector(response)</div><div class="line">        sites = sel.xpath(<span class="string">'//ul[@class="directory-url"]/li'</span>)</div><div class="line">        items = []</div><div class="line"></div><div class="line">        <span class="keyword">for</span> site <span class="keyword">in</span> sites:</div><div class="line">            item = DmozItem()</div><div class="line">            item[<span class="string">'name'</span>] = site.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">            item[<span class="string">'url'</span>] = site.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            item[<span class="string">'description'</span>] = site.xpath(<span class="string">'text()'</span>).re(<span class="string">'-\s[^\n]*\\r'</span>)</div><div class="line">            items.append(item)</div><div class="line">        <span class="keyword">return</span> items</div></pre></td></tr></table></figure>
<p>##3.5. 使用Item Pipeline </p>
<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。<br>每个item pipeline组件(有时称之为<code>ItemPipeline</code>)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。<br>以下是item pipeline的一些典型应用：</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬取的数据(检查item包含某些字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存，如保存到数据库、XML、JSON等文件中</li>
</ul>
<blockquote>
<p>　编写你自己的item pipeline很简单，每个item pipeline组件是一个独立的Python类，同时必须实现以下方法:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">process_item(item, spider)  <span class="comment">#每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item (或任何继承类)对象，或是抛出 DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理。</span></div><div class="line"><span class="comment">#参数:</span></div><div class="line">item: 由 parse 方法返回的 Item 对象(Item对象)</div><div class="line">spider: 抓取到这个 Item 对象对应的爬虫对象(Spider对象)</div><div class="line"></div><div class="line">open_spider(spider)  <span class="comment">#当spider被开启时，这个方法被调用。</span></div><div class="line"><span class="comment">#参数: </span></div><div class="line">spider : (Spider object) – 被开启的spider</div><div class="line">　　</div><div class="line">close_spider(spider)  <span class="comment">#当spider被关闭时，这个方法被调用，可以再爬虫关闭后进行相应的数据处理。</span></div><div class="line"><span class="comment">#参数: </span></div><div class="line">spider : (Spider object) – 被关闭的spider</div></pre></td></tr></table></figure>
<blockquote>
<p>为JSON文件编写一个items</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TutorialPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># put all words in lowercase</span></div><div class="line">    words_to_filter = [<span class="string">'politics'</span>, <span class="string">'religion'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> self.words_to_filter:</div><div class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> unicode(item[<span class="string">'description'</span>]).lower():</div><div class="line">                <span class="keyword">raise</span> DropItem(<span class="string">"Contains forbidden word: %s"</span> % word)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>在 settings.py 中设置<code>ITEM_PIPELINES</code>激活item pipeline，其默认为[]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123;&apos;tutorial.pipelines.FilterWordsPipeline&apos;: 1&#125;</div></pre></td></tr></table></figure>
<p>##3.6. 存储数据</p>
<p>使用下面的命令存储为<code>json</code>文件格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz -o items.json</div></pre></td></tr></table></figure>
<p>#4. Scarpy优化豆瓣爬虫的抓取</p>
<p>主要针对之间写过的豆瓣爬虫进行重构:</p>
<ul>
<li><a href="http://andrewliu.tk/2014/12/05/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E4%BA%8C-%E8%B1%86%E7%93%A3%E6%8A%93%E7%AB%99%E5%B0%8F%E8%AE%A1/" target="_blank" rel="external">Python网络爬虫(二)–豆瓣抓站小计</a></li>
<li><a href="http://andrewliu.tk/2014/12/15/Python-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BB%AD-Queue/" target="_blank" rel="external">豆瓣抓站重构第二版</a></li>
</ul>
<p>豆瓣有反爬虫机制, 只成功了一次后, 就被<code>baned</code>后显示403了, 下面说一下爬虫结构</p>
<p>##4.1. Item</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Item, Field</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanItem</span><span class="params">(Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    name = Field()  <span class="comment">#电影名称</span></div><div class="line">    description = Field()  <span class="comment">#电影描述</span></div><div class="line">    url = Field()  <span class="comment">#抓取的url</span></div></pre></td></tr></table></figure>
<p>##4.2. Spider主程序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="string">"""</span></div><div class="line">一个简单的Python 爬虫, 用于抓取豆瓣电影Top前250的电影的名称描述等</div><div class="line"></div><div class="line">Anthor: Andrew Liu</div><div class="line">Version: 0.0.3</div><div class="line">Date: 2014-12-17</div><div class="line">Language: Python2.7.8</div><div class="line">Editor: Sublime Text2</div><div class="line">Operate: 具体操作请看README.md介绍</div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</div><div class="line"><span class="keyword">from</span> douban.items <span class="keyword">import</span> DoubanItem</div><div class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors.sgml <span class="keyword">import</span> SgmlLinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanSpider</span><span class="params">(CrawlSpider)</span> :</span></div><div class="line"></div><div class="line">    name = <span class="string">"douban"</span> </div><div class="line">    allowed_domains = [<span class="string">"movie.douban.com"</span>]</div><div class="line">    start_urls = [<span class="string">"http://movie.douban.com/top250"</span>]</div><div class="line">    rules = (</div><div class="line">        <span class="comment">#将所有符合正则表达式的url加入到抓取列表中</span></div><div class="line">        Rule(SgmlLinkExtractor(allow = (<span class="string">r'http://movie\.douban\.com/top250\?start=\d+&amp;filter=&amp;type='</span>,))),</div><div class="line">        <span class="comment">#将所有符合正则表达式的url请求后下载网页代码, 形成response后调用自定义回调函数</span></div><div class="line">        Rule(SgmlLinkExtractor(allow = (<span class="string">r'http://movie\.douban\.com/subject/\d+'</span>, )), callback = <span class="string">'parse_page'</span>, follow = <span class="keyword">True</span>),</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span> :</span></div><div class="line">        sel = Selector(response)</div><div class="line">        item = DoubanItem()</div><div class="line">        item[<span class="string">'name'</span>] = sel.xpath(<span class="string">'//h1/span[@property="v:itemreviewed"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'description'</span>] = sel.xpath(<span class="string">'//div/span[@property="v:summary"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'url'</span>] = response.url</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>##4.3. 未来要解决的问题</p>
<ul>
<li>头部伪装</li>
<li>表单提交</li>
<li>编码转换</li>
</ul>
<blockquote>
<p>豆瓣抓了一会儿, 还没等我兴奋就被禁掉了</p>
</blockquote>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">2014-12-17 22:02:17+0800 [douban] DEBUG: Crawled (403) &lt;GET http://www.douban.com/misc/sorry?original-url=http%3A%2F%2Fmovie.douban.com%2Fsubject%2F2209573%2F%3Ffrom%3Dsubject-page&gt; (referer: http://movie.douban.com/subject/1849031/)</div><div class="line">2014-12-17 22:02:17+0800 [douban] DEBUG: Ignoring response &lt;403 http://www.douban.com/misc/sorry?original-url=http%3A%2F%2Fmovie.douban.com%2Fsubject%2F2209573%2F%3Ffrom%3Dsubject-page&gt;: HTTP status code is not handled or not allowed</div><div class="line">2014-12-17 22:02:17+0800 [douban] DEBUG: Crawled (403) &lt;GET http://www.douban.com/misc/sorry?original-url=http%3A%2F%2Fmovie.douban.com%2Fsubject%2F1849031%2Fcomments%3Fsort%3Dtime&gt; (referer: http://movie.douban.com/subject/1849031/)</div><div class="line">...</div></pre></td></tr></table></figure>
<p>#5. 参考链接</p>
<ul>
<li><a href="http://doc.scrapy.org/en/0.24/index.html#section-basics" target="_blank" rel="external">Basic concepts</a></li>
<li><a href="http://doc.scrapy.org/en/0.24/intro/overview.html#topics-whatelse" target="_blank" rel="external"> What else? section in Scrapy at a glance </a></li>
<li><a href="http://doc.scrapy.org/en/0.24/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="external">Item Pipeline</a></li>
<li><a href="http://doc.scrapy.org/en/0.24/intro/tutorial.html" target="_blank" rel="external">Scrapy Tutorial</a></li>
<li><a href="http://www.crummy.com/software/BeautifulSoup/bs3/documentation.zh.html" target="_blank" rel="external">BeautifulSoup</a></li>
<li><a href="http://wwwsearch.sourceforge.net/mechanize/" target="_blank" rel="external">mechanize</a></li>
<li><a href="http://scrapy.org/" target="_blank" rel="external">scrapy</a></li>
<li><a href="http://doc.scrapy.org/en/0.24/topics/selectors.html#topics-selectors" target="_blank" rel="external">Selectors documentation</a></li>
<li><a href="http://wsky.org/archives/191.html" target="_blank" rel="external">scrapy 中文教程（爬cnbeta实例）</a></li>
<li><a href="http://www.dmoz.org/" target="_blank" rel="external">dmoz</a></li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2014/12/19/IOS之UITabBarController/" class="prev">PREV</a><a href="/2014/12/15/Python-多线程续-Queue/" class="next">NEXT</a></div><div class="copyright"><p>© 2014 - 2017 <a href="http://andrewliu.in">Andrew Liu</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-58158116-2",'auto');ga('send','pageview');</script></body></html>