<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Python爬虫(一)--学习笔记 · Snow Memory | Andrew Liu</title><meta name="description" content="Python爬虫(一)--学习笔记 - Andrew Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://andrewliu.in/atom.xml" title="Snow Memory | Andrew Liu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/dinosaurliu" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Andrew-liu" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Python爬虫(一)--学习笔记</h1><div class="post-info">Dec 5, 2014</div><div class="post-content"><hr>
<p>#1. 初探urllib2模块</p>
<ul>
<li>浏览器相当于客户端, 向服务器发出资源请求(node.js充当的是服务器)</li>
<li><code>http</code>是基于请求和应答机制, 客户端发出请求, 服务器应答请求</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">response = urllib2.urlopen(<span class="string">"http://www.baidu.com/"</span>) <span class="comment">#使用urlopen进行url请求, 返回应答对象response</span></div><div class="line">html = response.read()  <span class="comment">#去读取应答对象的内容</span></div><div class="line"><span class="keyword">print</span> html</div><div class="line"></div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">req = urllib2.Request(<span class="string">"ftp://www.baidu.com/"</span>) <span class="comment">#使用url创建一个Request对象</span></div><div class="line">response = urllib2.urlopen(req)</div><div class="line">html = response.read()</div><div class="line"><span class="keyword">print</span> html</div></pre></td></tr></table></figure>
<a id="more"></a>
<blockquote>
<p>urllib2的接口可以处理所有的URL头, 如http, ftp</p>
</blockquote>
<ol>
<li>HTTPP请求时, 可以发送data表单数据</li>
<li>设置Headers到http请求(一些站点比喜欢被程序访问, 服务器确认请求的身份通过User-Agent头部, 创建一个请求对象时, 可以给它一个包含头数据的字典)</li>
</ol>
<blockquote>
<p>HTTPError可以作为页面返回的应答对象response,同样包含read,geturl和info方法</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="comment">#发送data表单数据</span></div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"><span class="keyword">import</span> urllib</div><div class="line"></div><div class="line">url = <span class="string">"http://www.someserver.com/register.cgi"</span></div><div class="line"></div><div class="line">values = &#123;</div><div class="line">    <span class="string">'name'</span> : <span class="string">'Andrew'</span>,</div><div class="line">    <span class="string">'location'</span> : <span class="string">'NJU'</span>,</div><div class="line">    <span class="string">'language'</span> : <span class="string">'Python'</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">#user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' </span></div><div class="line"><span class="comment">#header = &#123;'User-Agent' : user_agent&#125;</span></div><div class="line">data = urllib.urlencode(values) <span class="comment">#data数据需要编码成标准形式</span></div><div class="line"><span class="keyword">print</span> data</div><div class="line">req = urllib2.Request(url, data) <span class="comment">#发送请求同时传送data表单</span></div><div class="line"><span class="comment">#req = urllib2.Request(url, data, headers) #url 表单数据 伪装头部</span></div><div class="line">reponse = urllib2.urlopen(req) <span class="comment">#接受反馈数据</span></div><div class="line">html = reponse.read() <span class="comment">#读取反馈数据</span></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">full_url = url + '?' + data</div><div class="line">data = urllib2.open(full_url)</div><div class="line">"""</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#返回错误码, 200不是错误, 不会引起异常</span></div><div class="line">req = urllib2.Request(<span class="string">"http://bbs.csdn.net/callmewhy"</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    urllib2.urlopen(req)</div><div class="line"><span class="keyword">except</span> urllib2.HTTPError, e:</div><div class="line">    <span class="keyword">print</span> e.code</div><div class="line">    <span class="comment">#print e.read()</span></div></pre></td></tr></table></figure>
<ol>
<li>urlopen返回的应答对象response有两个常用方法<code>info()</code>和<code>geturl()</code></li>
</ol>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#查看重定向url的真实地址</div><div class="line">from urllib2 import Request, urlopen, URLError, HTTPError</div><div class="line"></div><div class="line">old_url = 'http://www.baidu.com'</div><div class="line">req = Request(old_url)</div><div class="line">response = urlopen(req) &amp;nbsp;</div><div class="line">print 'Old url :' + old_url</div><div class="line">print 'Real url :' + response.geturl()</div><div class="line"></div><div class="line"></div><div class="line">#输出:</div><div class="line">Old url :http://rrurl.cn/b1UZuP</div><div class="line">Real url :http://www.polyu.edu.hk/polyuchallenge/best_of_the_best_elevator_pitch_award/bbca_voting_process.php?voted_team_id=670&amp;section=1&amp;bbca_year_id=3</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#返回对象的字典, 字典描述了获取的页面情况, 通常是服务器发送的特定头headers</span></div><div class="line"><span class="keyword">from</span> urllib2 <span class="keyword">import</span> Request, urlopen, URLError, HTTPError</div><div class="line"></div><div class="line">old_url = <span class="string">'http://www.baidu.com'</span></div><div class="line">req = Request(old_url)</div><div class="line">response = urlopen(req)  </div><div class="line"><span class="keyword">print</span> <span class="string">'Info():'</span></div><div class="line"><span class="keyword">print</span> response.info()</div><div class="line"></div><div class="line"><span class="comment">#输出</span></div><div class="line">Info():</div><div class="line">Date: Fri, <span class="number">31</span> Oct <span class="number">2014</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">08</span> GMT</div><div class="line">Content-Type: text/html; charset=utf<span class="number">-8</span></div><div class="line">Transfer-Encoding: chunked</div><div class="line">Connection: Close</div><div class="line">Vary: Accept-Encoding</div><div class="line">Set-Cookie: BAIDUID=<span class="number">8</span>ACB98CA9413726C3133F5FFA1A24289:FG=<span class="number">1</span>; expires=Thu, <span class="number">31</span>-Dec<span class="number">-37</span> <span class="number">23</span>:<span class="number">55</span>:<span class="number">55</span> GMT; max-age=<span class="number">2147483647</span>; path=/; domain=.baidu.com</div><div class="line">...</div></pre></td></tr></table></figure>
<p>#2. 异常处理</p>
<blockquote>
<p>HTTPError是URLError的子类</p>
</blockquote>
<ol>
<li><code>URLError</code>在没有网络连接或者服务器不存在的情况下产生, 这种情况下异常会带有<code>reason</code>属性(不可变的tuple, 包含错误号和错误信息)</li>
<li><code>HTTPError</code>, 服务器上每个HTTP应答对象response包含一个<code>状态码</code><br>HTTP状态码标识HTTP协议所返回的响应状态; <code>HTTPError</code>有<code>code</code>属性,是服务器发送的相关错误号.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#HTTP状态码</span></div><div class="line"><span class="number">200</span>：请求成功      处理方式：获得响应的内容，进行处理 </div><div class="line"><span class="number">201</span>：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到    处理方式：爬虫中不会遇到 </div><div class="line"><span class="number">202</span>：请求被接受，但处理尚未完成    处理方式：阻塞等待 </div><div class="line"><span class="number">204</span>：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。    处理方式：丢弃</div><div class="line"><span class="number">300</span>：该状态码不被HTTP/<span class="number">1.0</span>的应用程序直接使用， 只是作为<span class="number">3</span>XX类型回应的默认解释。存在多个可用的被请求资源。    处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃</div><div class="line"><span class="number">301</span>：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源    处理方式：重定向到分配的URL</div><div class="line"><span class="number">302</span>：请求到的资源在一个不同的URL处临时保存     处理方式：重定向到临时的URL </div><div class="line"><span class="number">304</span> 请求的资源未更新     处理方式：丢弃 </div><div class="line"><span class="number">400</span> 非法请求     处理方式：丢弃 </div><div class="line"><span class="number">401</span> 未授权     处理方式：丢弃 </div><div class="line"><span class="number">403</span> 禁止     处理方式：丢弃 </div><div class="line"><span class="number">404</span> 没有找到     处理方式：丢弃 </div><div class="line"><span class="number">5</span>XX 回应代码以“<span class="number">5</span>”开头的状态码表示服务器端发现自己出现错误，不能继续执行请求    处理方式：丢弃</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line">req = urllib2.Request(<span class="string">"http://www.zhihu.com/"</span>)</div><div class="line"><span class="keyword">try</span> :</div><div class="line">    urllib2.urlopen(req)</div><div class="line"><span class="keyword">except</span> urllib2.URLError, e :</div><div class="line">    <span class="keyword">print</span> e.reason</div><div class="line"></div><div class="line"><span class="comment">#输出 :</span></div><div class="line">[Errno <span class="number">8</span>] nodename nor servname provided, <span class="keyword">or</span> <span class="keyword">not</span> known</div></pre></td></tr></table></figure>
<p>##2.1. 处理异常</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib2 <span class="keyword">import</span> Request, urlopen, URLError, HTTPError</div><div class="line"></div><div class="line">req = Request(<span class="string">'http://www.zhihu.com'</span>)</div><div class="line">  </div><div class="line"><span class="keyword">try</span>:  </div><div class="line">    response = urlopen(req)  </div><div class="line"><span class="keyword">except</span> URLError, e:  </div><div class="line">    <span class="keyword">if</span> hasattr(e, <span class="string">'code'</span>):  </div><div class="line">        <span class="keyword">print</span> <span class="string">'The server couldn\'t fulfill the request.'</span>  </div><div class="line">        <span class="keyword">print</span> <span class="string">'Error code: '</span>, e.code  </div><div class="line">    <span class="keyword">elif</span> hasattr(e, <span class="string">'reason'</span>):  </div><div class="line">        <span class="keyword">print</span> <span class="string">'We failed to reach a server.'</span>  </div><div class="line">        <span class="keyword">print</span> <span class="string">'Reason: '</span>, e.reason  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">    <span class="keyword">print</span> <span class="string">'No exception was raised.'</span>  </div><div class="line">    <span class="comment"># everything is fine</span></div></pre></td></tr></table></figure>
<p>#3. Openers和Handlers</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line"><span class="comment">#创建一个密码管理者</span></div><div class="line">password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()</div><div class="line"></div><div class="line"><span class="comment">#添加用户名和密码</span></div><div class="line">top_level_url = <span class="string">"http://example.com/foo/"</span></div><div class="line"></div><div class="line"><span class="comment">#如果知道realm, 可以用来代替None</span></div><div class="line">password_mgr.add_password(<span class="keyword">None</span>, top_level_url, <span class="string">'username'</span>, <span class="string">'password'</span>)</div><div class="line"></div><div class="line"><span class="comment">#创建一个新的Hanlder</span></div><div class="line">handler = urllib2.HTTPBasicAuthHandler(password_mgr)</div><div class="line"></div><div class="line"><span class="comment">#创建opener</span></div><div class="line">opener = urllib2.build_opener(handler)</div><div class="line"></div><div class="line">a_url = <span class="string">'http://www.baidu.com/'</span></div><div class="line"></div><div class="line"><span class="comment">#使用opener获取一个url</span></div><div class="line">opener.open(a_url)</div><div class="line"></div><div class="line"><span class="comment">#安装opener, 此后调用urrlib2.urlopen将用自定义opener</span></div><div class="line">urllib2.install_opener(opener)</div></pre></td></tr></table></figure>
<p>#4. urllib2的使用细节</p>
<p>##4.1. Proxy代理的设置</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">class urllib2.ProxyHandler([proxies])</div><div class="line">Cause requests to go through a proxy. If proxies is given, it must be a dictionary mapping protocol names to URLs of proxies. </div><div class="line"></div><div class="line">urllib2.install_opener(opener)</div><div class="line">Install an OpenerDirector instance as the default global opener. Installing an opener is only necessary if you want urlopen to use that opener</div><div class="line"></div><div class="line">urllib2.build_opener([handler, ...])</div><div class="line">Return an OpenerDirector instance, which chains the handlers in the order given. handlers can be either instances of BaseHandler, or subclasses of BaseHandler (in which case it must be possible to call the constructor without any parameters).</div></pre></td></tr></table></figure>
<p>urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy, 程序中明确控制 Proxy 而不受环境变量的影响，可以使用代理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">enable_proxy = <span class="keyword">True</span></div><div class="line">proxy_handler = urllib2.ProxyHandler(&#123;<span class="string">"http"</span> : <span class="string">'http://some-proxy.com:8080'</span>&#125;)</div><div class="line">null_proxy_handler = urllib2.ProxyHandler(&#123;&#125;)</div><div class="line"><span class="keyword">if</span> enable_proxy:</div><div class="line">    opener = urllib2.build_opener(proxy_handler)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    opener = urllib2.build_opener(null_proxy_handler)</div><div class="line">urllib2.install_opener(opener)  <span class="comment">#urllib2.install_opener() 会设置 urllib2 的全局 opener</span></div></pre></td></tr></table></figure>
<p>##4.2. Timeout设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">The optional timeout parameter specifies a timeout <span class="keyword">in</span> seconds <span class="keyword">for</span> blocking operations like the connection attempt (<span class="keyword">if</span> <span class="keyword">not</span> specified, the <span class="keyword">global</span> default timeout setting will be used). This actually only works <span class="keyword">for</span> HTTP, HTTPS <span class="keyword">and</span> FTP connections</div><div class="line"></div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">response = urllib2.urlopen(<span class="string">'http://www.google.com'</span>, timeout=<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>##4.3. 加入特定的Header</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line">request = urllib2.Request(&apos;http://www.baidu.com/&apos;)</div><div class="line">request.add_header(&apos;User-Agent&apos;, &apos;fake-client&apos;)</div><div class="line">response = urllib2.urlopen(request)</div><div class="line">print response.read()</div><div class="line"></div><div class="line"></div><div class="line">#或者直接使用</div><div class="line">request = urllib2.Request(&apos;http://www.baidu.com/&apos;, &#123;&apos;User-Agent&apos;: &apos;fake-client&apos;&#125;)</div></pre></td></tr></table></figure>
<p>注意特殊的Header</p>
<ul>
<li>User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求</li>
<li>Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。常见的取值有：</li>
<li>application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用</li>
<li>application/json ： 在 JSON RPC 调用时使用</li>
<li>application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用<br>在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务</li>
</ul>
<p>##4.4. Redirect</p>
<p>只要检查一下 Response 的 URL 和 Request 的 URL 是否一致就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">my_url = <span class="string">'http://www.baidu.cn'</span></div><div class="line">response = urllib2.urlopen(my_url)</div><div class="line">redirected = response.geturl() == my_url</div><div class="line"><span class="keyword">print</span> redirected</div><div class="line"></div><div class="line">my_url = <span class="string">'http://rrurl.cn/b1UZuP'</span></div><div class="line">response = urllib2.urlopen(my_url)</div><div class="line">redirected = response.geturl() == my_url</div><div class="line"><span class="keyword">print</span> redirected</div></pre></td></tr></table></figure>
<p>如果不想自动 redirect，除了使用更低层次的 httplib 库之外，还可以自定义<code>HTTPRedirectHandler</code> 类</p>
<p>##4.5. Cookie</p>
<p>urllib2 对 Cookie 的处理也是自动的。如果需要得到某个 Cookie 项的值, 可以像下面一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"><span class="keyword">import</span> cookielib</div><div class="line">cookie = cookielib.CookieJar()</div><div class="line">opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))</div><div class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</div><div class="line">    <span class="keyword">print</span> <span class="string">'Name = '</span>+item.name</div><div class="line">    <span class="keyword">print</span> <span class="string">'Value = '</span>+item.value</div><div class="line">    </div><div class="line">Name = BAIDUID</div><div class="line">Value = <span class="number">6</span>AD640C70CF0772950084D47494ADF9F:FG=<span class="number">1</span></div><div class="line">Name = BAIDUPSID</div><div class="line">Value = <span class="number">6</span>AD640C70CF0772950084D47494ADF9F</div><div class="line">Name = H_PS_PSSID</div><div class="line">Value = <span class="number">5015</span>_10159_1442_9591_7800_10194_10251_9932_10211_10120_10210_10016_10178_9498_10051_10066_9770_10096_10008_10235_10257_9978_9023</div><div class="line">Name = BDSVRTM</div><div class="line">Value = <span class="number">0</span></div><div class="line">Name = BD_HOME</div><div class="line">Value = <span class="number">0</span></div></pre></td></tr></table></figure>
<p>##4.6. 得到 HTTP 的返回码</p>
<p>对于正常访问的网页,urlopen 返回的 response 对象的 getcode() 方法就可以得到 HTTP 的返回码。但对其它返回码来说，urlopen 会抛出异常。这时候，就要检查异常对象的 code 属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_code</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        response = urllib2.urlopen(<span class="string">"http://www.baidu.com"</span>)</div><div class="line">    <span class="keyword">except</span> urllib2.HTTPError, e:</div><div class="line">        <span class="keyword">print</span> e.code</div><div class="line">    <span class="keyword">print</span> response.getcode()</div></pre></td></tr></table></figure>
<p>##4.7. Debug log<br>使用 urllib2 时，可以通过下面的方法把 debug Log 打开，这样收发包的内容就会在屏幕上打印出来，方便调试，有时可以省去抓包的工作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug_log</span><span class="params">()</span>:</span></div><div class="line">    http_handler = urllib2.HTTPHandler(debuglevel = <span class="number">1</span>)</div><div class="line">    https_handler = urllib2.HTTPSHandler(debuglevel = <span class="number">1</span>)</div><div class="line">    opener = urllib2.build_opener(http_handler, https_handler)</div><div class="line">    urllib2.install_opener(opener)</div><div class="line">    response = urllib2.urlopen(<span class="string">"http://www.baidu.com"</span>)</div><div class="line"></div><div class="line">send: <span class="string">'GET / HTTP/1.1\r\nAccept-Encoding: identity\r\nHost: www.baidu.com\r\nConnection: close\r\nUser-Agent: Python-urllib/2.7\r\n\r\n'</span></div><div class="line">reply: <span class="string">'HTTP/1.1 200 OK\r\n'</span></div><div class="line">header: Date: Sat, <span class="number">29</span> Nov <span class="number">2014</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">32</span> GMT</div><div class="line">header: Content-Type: text/html; charset=utf<span class="number">-8</span></div><div class="line">header: Transfer-Encoding: chunked</div><div class="line">header: Connection: Close</div><div class="line">header: Vary: Accept-Encoding</div><div class="line">header: Set-Cookie: BAIDUID=<span class="number">8</span>CD4DE8C39725B1B6C054E5211DCA422:FG=<span class="number">1</span>; expires=Thu, <span class="number">31</span>-Dec<span class="number">-37</span> <span class="number">23</span>:<span class="number">55</span>:<span class="number">55</span> GMT; max-age=<span class="number">2147483647</span>; path=/; domain=.baidu.com</div><div class="line">header: Set-Cookie: BAIDUPSID=<span class="number">8</span>CD4DE8C39725B1B6C054E5211DCA422; expires=Thu, <span class="number">31</span>-Dec<span class="number">-37</span> <span class="number">23</span>:<span class="number">55</span>:<span class="number">55</span> GMT; max-age=<span class="number">2147483647</span>; path=/; domain=.baidu.com</div><div class="line">header: Set-Cookie: BDSVRTM=<span class="number">0</span>; path=/</div><div class="line">header: Set-Cookie: BD_HOME=<span class="number">0</span>; path=/</div><div class="line">header: Set-Cookie: H_PS_PSSID=<span class="number">10148</span>_10204_10160_1423_9993_7802_10194_9475_10121_10209_10016_10179_9499_10051_10218_10065_9769_10096_10007_10236_10257_9978_9024; path=/; domain=.baidu.com</div><div class="line">header: P3P: CP=<span class="string">" OTI DSP COR IVA OUR IND COM "</span></div><div class="line">header: Cache-Control: private</div><div class="line">header: Cxy_all: baidu+d0a19f3803227c13f67815859a8916ed</div><div class="line">header: Expires: Sat, <span class="number">29</span> Nov <span class="number">2014</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">06</span> GMT</div><div class="line">header: X-Powered-By: HPHP</div><div class="line">header: Server: BWS/<span class="number">1.1</span></div><div class="line">header: BDPAGETYPE: <span class="number">1</span></div><div class="line">header: BDQID: <span class="number">0xdb113cef0017ad4f</span></div><div class="line">header: BDUSERID: <span class="number">0</span></div></pre></td></tr></table></figure>
<p>##4.8. 表单的处理</p>
<p>用firefox+httpfox插件来看看自己到底发送了些什么包。<br>先找到自己发的POST请求，以及POST表单项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line">postdata=urllib.urlencode(&#123;</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; <span class="string">'username'</span>:<span class="string">'汪小光'</span>,</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; <span class="string">'password'</span>:<span class="string">'why888'</span>,</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; <span class="string">'continueURI'</span>:<span class="string">'http://www.verycd.com/'</span>,</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; <span class="string">'fk'</span>:<span class="string">''</span>,</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; <span class="string">'login_submit'</span>:<span class="string">'登录'</span></div><div class="line">&#125;)</div><div class="line">req = urllib2.Request(</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; url = <span class="string">'http://secure.verycd.com/signin'</span>,</div><div class="line">&amp;nbsp;&amp;nbsp;&amp;nbsp; data = postdata</div><div class="line">)</div><div class="line">result = urllib2.urlopen(req)</div><div class="line"><span class="keyword">print</span> result.read()</div></pre></td></tr></table></figure>
<p>##4.9. 伪装成浏览器访问</p>
<p>某些网站反感爬虫的到访，于是对爬虫一律拒绝请求<br>这时候我们需要伪装成浏览器，这可以通过修改http包中的header来实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">imitate_browser</span><span class="params">()</span>:</span></div><div class="line">    post_data = urllib.urlencode(&#123;</div><div class="line">        <span class="string">'username'</span>: <span class="string">'汪小光'</span>,</div><div class="line">        <span class="string">'password'</span>: <span class="string">'why888'</span>,</div><div class="line">        <span class="string">'continueURI'</span>: <span class="string">'http://www.verycd.com/'</span>,</div><div class="line">        <span class="string">'fk'</span>: <span class="string">''</span>,</div><div class="line">        <span class="string">'login_submit'</span>: <span class="string">'登陆'</span></div><div class="line">    &#125;)</div><div class="line">    headers = &#123;</div><div class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span> </div><div class="line">    &#125;</div><div class="line">    req = urllib2.Request(</div><div class="line">    url = <span class="string">'http://secure.verycd.com/signin/*/http://www.verycd.com/'</span>,</div><div class="line">    data = post_data, </div><div class="line">    headers = headers</div><div class="line">    )</div><div class="line">    result = urllib2.urlopen(req)</div><div class="line">    <span class="keyword">print</span> result.read()</div></pre></td></tr></table></figure>
<p>##4.10. 对付”反盗链”</p>
<p>某些站点有所谓的反盗链设置，其实说穿了很简单，<br>就是检查你发送请求的header里面，referer站点是不是他自己，<br>所以我们只需要像把headers的referer改成该网站即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reverse_link</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"></div><div class="line">    反盗链, 查看头部referer是否为访问的网站</div><div class="line">    """</div><div class="line">    post_data = urllib.urlencode(&#123;</div><div class="line">        <span class="string">'username'</span>: <span class="string">'汪小光'</span>,</div><div class="line">        <span class="string">'password'</span>: <span class="string">'why888'</span>,</div><div class="line">        <span class="string">'continueURI'</span>: <span class="string">'http://www.verycd.com/'</span>,</div><div class="line">        <span class="string">'fk'</span>: <span class="string">''</span>,</div><div class="line">        <span class="string">'login_submit'</span>: <span class="string">'登陆'</span></div><div class="line">    &#125;)</div><div class="line">    headers = &#123;</div><div class="line">    <span class="string">'Referer'</span>: <span class="string">'http://www.cnbeta.com/articles'</span></div><div class="line">    &#125;</div><div class="line">    req = urllib2.Request(</div><div class="line">    url = <span class="string">'http://secure.verycd.com/signin/*/http://www.verycd.com/'</span>,</div><div class="line">    data = post_data, </div><div class="line">    headers = headers</div><div class="line">    )</div><div class="line">    result = urllib2.urlopen(req)</div><div class="line">    <span class="keyword">print</span> result.read()</div></pre></td></tr></table></figure>
<blockquote>
<p>本文为学习<a href="http://blog.csdn.net/column/details/why-bug.html" target="_blank" rel="external">Python爬虫入门教程</a>笔记</p>
</blockquote>
</div></article></div></main><footer><div class="paginator"><a href="/2014/12/05/Python网络爬虫-二-豆瓣抓站小计/" class="prev">上一篇</a><a href="/2014/12/01/IOS之View管理/" class="next">下一篇</a></div><div class="copyright"><p>© 2014 - 2017 <a href="http://andrewliu.in">Andrew Liu</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-58158116-2",'auto');ga('send','pageview');</script></body></html>