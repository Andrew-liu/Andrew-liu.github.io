<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Python爬虫(八)--Scrapy模拟登录 · Snow Memory | Andrew Liu</title><meta name="description" content="Python爬虫(八)--Scrapy模拟登录 - Andrew Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://andrewliu.in/atom.xml" title="Snow Memory | Andrew Liu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/dinosaurliu" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Andrew-liu" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Python爬虫(八)--Scrapy模拟登录</h1><div class="post-info">Dec 19, 2014</div><div class="post-content"><h2 id="1-Cookie原理"><a href="#1-Cookie原理" class="headerlink" title="#1. Cookie原理"></a>#1. Cookie原理</h2><blockquote>
<p>HTTP是无状态的面向连接的协议, 为了保持连接状态, 引入了Cookie机制</p>
</blockquote>
<p>Cookie是http消息头中的一种属性，包括：</p>
<ul>
<li>Cookie名字（Name）Cookie的值（Value）</li>
<li>Cookie的过期时间（Expires/Max-Age）</li>
<li>Cookie作用路径（Path）</li>
<li>Cookie所在域名（Domain），使用Cookie进行安全连接（Secure）。<br>前两个参数是Cookie应用的必要条件，另外，还包括Cookie大小（Size，不同浏览器对Cookie个数及大小限制是有差异的）。</li>
</ul>
<p><a href="http://en.wikipedia.org/wiki/HTTP_cookie" target="_blank" rel="external">更详细的cookie</a></p>
<a id="more"></a>
<h2 id="2-模拟登陆"><a href="#2-模拟登陆" class="headerlink" title="#2. 模拟登陆"></a>#2. 模拟登陆</h2><blockquote>
<p>这次主要爬取的网站是<a href="http://www.zhihu.com" target="_blank" rel="external">知乎</a></p>
</blockquote>
<p>爬取知乎就需要登陆的, 通过之前的python内建库, 可以很容易的实现表单提交</p>
<p><strong>现在就来看看如何通过Scrapy实现表单提交</strong></p>
<p>首先查看登陆时的表单结果, 依然像前面使用的技巧一样, 故意输错密码, 方面抓到登陆的网页头部和表单(<code>我使用的Chrome自带的开发者工具中的Network功能</code>)</p>
<p><img src="http://7rfjyu.com1.z0.glb.clouddn.com/Snip20141219_1.png" alt="表单截图"></p>
<p>查看抓取到的表单可以发现有四个部分:</p>
<ul>
<li>邮箱和密码就是个人登陆的邮箱和密码</li>
<li>rememberme字段表示是否记住账号</li>
<li>第一个字段是<code>_xsrf</code>,猜测是一种验证机制</li>
</ul>
<blockquote>
<p>现在只有<code>_xsrf</code>不知道, 猜想这个验证字段肯定会实现在请求网页的时候发送过来, 那么我们查看当前网页的源码(鼠标右键然后查看网页源代码, 或者直接用快捷键)</p>
</blockquote>
<p><img src="http://7rfjyu.com1.z0.glb.clouddn.com/Snip20141219_2.png" alt="查询网页源码"></p>
<p>发现我们的猜测是正确的</p>
<p>那么现在就可以来写表单登陆功能了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> [Request(<span class="string">"https://www.zhihu.com/login"</span>, callback = self.post_login)]  <span class="comment">#重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数</span></div><div class="line"></div><div class="line">    <span class="comment">#FormRequeset</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Preparing login'</span></div><div class="line">        <span class="comment">#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单</span></div><div class="line">        xsrf = Selector(response).xpath(<span class="string">'//input[@name="_xsrf"]/@value'</span>).extract()[<span class="number">0</span>]</div><div class="line">        <span class="keyword">print</span> xsrf</div><div class="line">        <span class="comment">#FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></div><div class="line">        <span class="comment">#登陆成功后, 会调用after_login回调函数</span></div><div class="line">        <span class="keyword">return</span> [FormRequest.from_response(response,   </div><div class="line">                            formdata = &#123;</div><div class="line">                            <span class="string">'_xsrf'</span>: xsrf,</div><div class="line">                            <span class="string">'email'</span>: <span class="string">'123456'</span>,</div><div class="line">                            <span class="string">'password'</span>: <span class="string">'123456'</span></div><div class="line">                            &#125;,</div><div class="line">                            callback = self.after_login</div><div class="line">                            )]</div></pre></td></tr></table></figure>
<blockquote>
<p>其中主要的功能都在函数的注释中说明</p>
</blockquote>
<h2 id="3-Cookie的保存"><a href="#3-Cookie的保存" class="headerlink" title="#3. Cookie的保存"></a>#3. Cookie的保存</h2><p>为了能使用同一个状态持续的爬取网站, 就需要保存<code>cookie</code>, 使用cookie保存状态, <code>Scrapy</code>提供了cookie处理的中间件, 可以直接拿来使用</p>
<p><a href="http://doc.scrapy.org/en/0.24/topics/downloader-middleware.html?highlight=cookie#module-scrapy.contrib.downloadermiddleware.cookies" target="_blank" rel="external">CookiesMiddleware</a></p>
<blockquote>
<p>这个cookie中间件保存追踪web服务器发出的cookie, 并将这个cookie在接来下的请求的时候进行发送</p>
</blockquote>
<p>Scrapy官方的文档中给出了下面的代码范例 :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i, url <span class="keyword">in</span> enumerate(urls):</div><div class="line">    <span class="keyword">yield</span> scrapy.Request(<span class="string">"http://www.example.com"</span>, meta=&#123;<span class="string">'cookiejar'</span>: i&#125;,</div><div class="line">        callback=self.parse_page)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="comment"># do some processing</span></div><div class="line">    <span class="keyword">return</span> scrapy.Request(<span class="string">"http://www.example.com/otherpage"</span>,</div><div class="line">        meta=&#123;<span class="string">'cookiejar'</span>: response.meta[<span class="string">'cookiejar'</span>]&#125;,</div><div class="line">        callback=self.parse_other_page)</div></pre></td></tr></table></figure>
<p>那么可以对我们的爬虫类中方法进行修改, 使其追踪cookie</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> [Request(<span class="string">"https://www.zhihu.com/login"</span>, meta = &#123;<span class="string">'cookiejar'</span> : <span class="number">1</span>&#125;, callback = self.post_login)]  <span class="comment">#添加了meta</span></div><div class="line"></div><div class="line"><span class="comment">#FormRequeset出问题了</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Preparing login'</span></div><div class="line">    <span class="comment">#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单</span></div><div class="line">    xsrf = Selector(response).xpath(<span class="string">'//input[@name="_xsrf"]/@value'</span>).extract()[<span class="number">0</span>]</div><div class="line">    <span class="keyword">print</span> xsrf</div><div class="line">    <span class="comment">#FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></div><div class="line">    <span class="comment">#登陆成功后, 会调用after_login回调函数</span></div><div class="line">    <span class="keyword">return</span> [FormRequest.from_response(response,   <span class="comment">#"http://www.zhihu.com/login",</span></div><div class="line">                        meta = &#123;<span class="string">'cookiejar'</span> : response.meta[<span class="string">'cookiejar'</span>]&#125;, <span class="comment">#注意这里cookie的获取</span></div><div class="line">                        headers = self.headers,</div><div class="line">                        formdata = &#123;</div><div class="line">                        <span class="string">'_xsrf'</span>: xsrf,</div><div class="line">                        <span class="string">'email'</span>: <span class="string">'123456'</span>,</div><div class="line">                        <span class="string">'password'</span>: <span class="string">'123456'</span></div><div class="line">                        &#125;,</div><div class="line">                        callback = self.after_login,</div><div class="line">                        dont_filter = <span class="keyword">True</span></div><div class="line">                        )]</div></pre></td></tr></table></figure>
<h2 id="4-伪装头部"><a href="#4-伪装头部" class="headerlink" title="#4. 伪装头部"></a>#4. 伪装头部</h2><p>有时候登陆网站需要进行头部伪装, 比如增加防盗链的头部, 还有模拟服务器登陆, 这些都在前面的爬虫知识中提到过</p>
<p><img src="http://7rfjyu.com1.z0.glb.clouddn.com/Snip20141219_3.png" alt="Headers"></p>
<p>为了保险, 我们可以在头部中填充更多的字段, 如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">headers = &#123;</div><div class="line">&quot;Accept&quot;: &quot;*/*&quot;,</div><div class="line">&quot;Accept-Encoding&quot;: &quot;gzip,deflate&quot;,</div><div class="line">&quot;Accept-Language&quot;: &quot;en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4&quot;,</div><div class="line">&quot;Connection&quot;: &quot;keep-alive&quot;,</div><div class="line">&quot;Content-Type&quot;:&quot; application/x-www-form-urlencoded; charset=UTF-8&quot;,</div><div class="line">&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36&quot;,</div><div class="line">&quot;Referer&quot;: &quot;http://www.zhihu.com/&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在scrapy中<code>Request</code>和<code>FormRequest</code>初始化的时候都有一个headers字段, 可以自定义头部, 这样我们可以添加headers字段</p>
<p>形成最终版的登陆函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding:utf-8 -*-</span></div><div class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</div><div class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors.sgml <span class="keyword">import</span> SgmlLinkExtractor</div><div class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request, FormRequest</div><div class="line"><span class="keyword">from</span> zhihu.items <span class="keyword">import</span> ZhihuItem</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSipder</span><span class="params">(CrawlSpider)</span> :</span></div><div class="line">    name = <span class="string">"zhihu"</span></div><div class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.zhihu.com"</span></div><div class="line">    ]</div><div class="line">    rules = (</div><div class="line">        Rule(SgmlLinkExtractor(allow = (<span class="string">'/question/\d+#.*?'</span>, )), callback = <span class="string">'parse_page'</span>, follow = <span class="keyword">True</span>),</div><div class="line">        Rule(SgmlLinkExtractor(allow = (<span class="string">'/question/\d+'</span>, )), callback = <span class="string">'parse_page'</span>, follow = <span class="keyword">True</span>),</div><div class="line">    )</div><div class="line">    headers = &#123;</div><div class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>,</div><div class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip,deflate"</span>,</div><div class="line">    <span class="string">"Accept-Language"</span>: <span class="string">"en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4"</span>,</div><div class="line">    <span class="string">"Connection"</span>: <span class="string">"keep-alive"</span>,</div><div class="line">    <span class="string">"Content-Type"</span>:<span class="string">" application/x-www-form-urlencoded; charset=UTF-8"</span>,</div><div class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36"</span>,</div><div class="line">    <span class="string">"Referer"</span>: <span class="string">"http://www.zhihu.com/"</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">#重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> [Request(<span class="string">"https://www.zhihu.com/login"</span>, meta = &#123;<span class="string">'cookiejar'</span> : <span class="number">1</span>&#125;, callback = self.post_login)]</div><div class="line"></div><div class="line">    <span class="comment">#FormRequeset出问题了</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Preparing login'</span></div><div class="line">        <span class="comment">#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单</span></div><div class="line">        xsrf = Selector(response).xpath(<span class="string">'//input[@name="_xsrf"]/@value'</span>).extract()[<span class="number">0</span>]</div><div class="line">        <span class="keyword">print</span> xsrf</div><div class="line">        <span class="comment">#FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></div><div class="line">        <span class="comment">#登陆成功后, 会调用after_login回调函数</span></div><div class="line">        <span class="keyword">return</span> [FormRequest.from_response(response,   <span class="comment">#"http://www.zhihu.com/login",</span></div><div class="line">                            meta = &#123;<span class="string">'cookiejar'</span> : response.meta[<span class="string">'cookiejar'</span>]&#125;,</div><div class="line">                            headers = self.headers,  <span class="comment">#注意此处的headers</span></div><div class="line">                            formdata = &#123;</div><div class="line">                            <span class="string">'_xsrf'</span>: xsrf,</div><div class="line">                            <span class="string">'email'</span>: <span class="string">'123456'</span>,</div><div class="line">                            <span class="string">'password'</span>: <span class="string">'123456'</span></div><div class="line">                            &#125;,</div><div class="line">                            callback = self.after_login,</div><div class="line">                            dont_filter = <span class="keyword">True</span></div><div class="line">                            )]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span> :</span></div><div class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls :</div><div class="line">            <span class="keyword">yield</span> self.make_requests_from_url(url)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span>:</span></div><div class="line">        problem = Selector(response)</div><div class="line">        item = ZhihuItem()</div><div class="line">        item[<span class="string">'url'</span>] = response.url</div><div class="line">        item[<span class="string">'name'</span>] = problem.xpath(<span class="string">'//span[@class="name"]/text()'</span>).extract()</div><div class="line">        <span class="keyword">print</span> item[<span class="string">'name'</span>]</div><div class="line">        item[<span class="string">'title'</span>] = problem.xpath(<span class="string">'//h2[@class="zm-item-title zm-editable-content"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'description'</span>] = problem.xpath(<span class="string">'//div[@class="zm-editable-content"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'answer'</span>]= problem.xpath(<span class="string">'//div[@class=" zm-editable-content clearfix"]/text()'</span>).extract()</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="5-Item类和抓取间隔"><a href="#5-Item类和抓取间隔" class="headerlink" title="#5. Item类和抓取间隔 "></a>#5. Item类和抓取间隔 </h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">from scrapy.item import Item, Field</div><div class="line"></div><div class="line"></div><div class="line">class ZhihuItem(Item):</div><div class="line">    # define the fields for your item here like:</div><div class="line">    # name = scrapy.Field()</div><div class="line">    url = Field()  #保存抓取问题的url</div><div class="line">    title = Field()  #抓取问题的标题</div><div class="line">    description = Field()  #抓取问题的描述</div><div class="line">    answer = Field()  #抓取问题的答案</div><div class="line">    name = Field()  #个人用户的名称</div></pre></td></tr></table></figure>
<p>设置抓取间隔, 访问由于爬虫的过快抓取, 引发网站的发爬虫机制, 在<code>setting.py</code>中设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">BOT_NAME = <span class="string">'zhihu'</span></div><div class="line"></div><div class="line">SPIDER_MODULES = [<span class="string">'zhihu.spiders'</span>]</div><div class="line">NEWSPIDER_MODULE = <span class="string">'zhihu.spiders'</span></div><div class="line">DOWNLOAD_DELAY = <span class="number">0.25</span>   <span class="comment">#设置下载间隔为250ms</span></div></pre></td></tr></table></figure>
<p><a href="http://scrapy-chs.readthedocs.org/zh_CN/latest/topics/settings.html" target="_blank" rel="external">更多设置可以查看官方文档</a></p>
<p>抓取结果(<code>只是截取了其中很少一部分</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"> <span class="string">'url'</span>: <span class="string">'http://www.zhihu.com/question/20688855/answer/16577390'</span>&#125;</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-19</span> <span class="number">23</span>:<span class="number">24</span>:<span class="number">15</span>+<span class="number">0800</span> [zhihu] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://www.zhihu.com/question/<span class="number">20688855</span>/answer/<span class="number">15861368</span>&gt; (referer: http://www.zhihu.com/question/<span class="number">20688855</span>/answer/<span class="number">19231794</span>)</div><div class="line">[]</div><div class="line"><span class="number">2014</span><span class="number">-12</span><span class="number">-19</span> <span class="number">23</span>:<span class="number">24</span>:<span class="number">15</span>+<span class="number">0800</span> [zhihu] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://www.zhihu.com/question/<span class="number">20688855</span>/answer/<span class="number">15861368</span>&gt;</div><div class="line">    &#123;<span class="string">'answer'</span>: [<span class="string">u'\u9009\u4f1a\u8ba1\u8fd9\u4e2a\u4e13\u4e1a\uff0c\u8003CPA\uff0c\u5165\u8d22\u52a1\u8fd9\u4e2a\u884c\u5f53\u3002\u8fd9\u4e00\u8def\u8d70\u4e0b\u6765\uff0c\u6211\u53ef\u4ee5\u5f88\u80af\u5b9a\u7684\u544a\u8bc9\u4f60\uff0c\u6211\u662f\u771f\u7684\u559c\u6b22\u8d22\u52a1\uff0c\u70ed\u7231\u8fd9\u4e2a\u884c\u4e1a\uff0c\u56e0\u6b64\u575a\u5b9a\u4e0d\u79fb\u5730\u5728\u8fd9\u4e2a\u884c\u4e1a\u4e2d\u8d70\u4e0b\u53bb\u3002'</span>,</div><div class="line">                <span class="string">u'\u4e0d\u8fc7\u4f60\u8bf4\u6709\u4eba\u4ece\u5c0f\u5c31\u559c\u6b22\u8d22\u52a1\u5417\uff1f\u6211\u89c9\u5f97\u51e0\u4e4e\u6ca1\u6709\u5427\u3002\u8d22\u52a1\u7684\u9b45\u529b\u5728\u4e8e\u4f60\u771f\u6b63\u61c2\u5f97\u5b83\u4e4b\u540e\u3002'</span>,</div><div class="line">                <span class="string">u'\u901a\u8fc7\u5b83\uff0c\u4f60\u53ef\u4ee5\u5b66\u4e60\u4efb\u4f55\u4e00\u79cd\u5546\u4e1a\u7684\u7ecf\u8425\u8fc7\u7a0b\uff0c\u4e86\u89e3\u5176\u7eb7\u7e41\u5916\u8868\u4e0b\u7684\u5b9e\u7269\u6d41\u3001\u73b0\u91d1\u6d41\uff0c\u751a\u81f3\u4f60\u53ef\u4ee5\u638c\u63e1\u5982\u4f55\u53bb\u7ecf\u8425\u8fd9\u79cd\u5546\u4e1a\u3002'</span>,</div><div class="line">                <span class="string">u'\u5982\u679c\u5bf9\u4f1a\u8ba1\u7684\u8ba4\u8bc6\u4ec5\u4ec5\u505c\u7559\u5728\u505a\u5206\u5f55\u8fd9\u4e2a\u5c42\u9762\uff0c\u5f53\u7136\u4f1a\u89c9\u5f97\u67af\u71e5\u65e0\u5473\u3002\u5f53\u4f60\u5bf9\u5b83\u7684\u8ba4\u8bc6\u8fdb\u5165\u5230\u6df1\u5c42\u6b21\u7684\u65f6\u5019\uff0c\u4f60\u81ea\u7136\u5c31\u4f1a\u559c\u6b22\u4e0a\u5b83\u4e86\u3002\n\n\n'</span>],</div><div class="line">     <span class="string">'description'</span>: [<span class="string">u'\u672c\u4eba\u5b66\u4f1a\u8ba1\u6559\u80b2\u4e13\u4e1a\uff0c\u6df1\u611f\u5176\u67af\u71e5\u4e4f\u5473\u3002\n\u5f53\u521d\u662f\u51b2\u7740\u5e08\u8303\u4e13\u4e1a\u62a5\u7684\uff0c\u56e0\u4e3a\u68a6\u60f3\u662f\u6210\u4e3a\u4e00\u540d\u8001\u5e08\uff0c\u4f46\u662f\u611f\u89c9\u73b0\u5728\u666e\u901a\u521d\u9ad8\u4e2d\u8001\u5e08\u5df2\u7ecf\u8d8b\u4e8e\u9971\u548c\uff0c\u800c\u987a\u6bcd\u4eb2\u5927\u4eba\u7684\u610f\u9009\u4e86\u8fd9\u4e2a\u4e13\u4e1a\u3002\u6211\u559c\u6b22\u4e0a\u6559\u80b2\u5b66\u7684\u8bfe\uff0c\u5e76\u597d\u7814\u7a76\u5404\u79cd\u6559\u80b2\u5fc3\u7406\u5b66\u3002\u4f46\u4f1a\u8ba1\u8bfe\u4f3c\u4e4e\u662f\u4e3b\u6d41\u3001\u54ce\u3002\n\n\u4e00\u76f4\u4e0d\u559c\u6b22\u94b1\u4e0d\u94b1\u7684\u4e13\u4e1a\uff0c\u6240\u4ee5\u5f88\u597d\u5947\u5927\u5bb6\u9009\u4f1a\u8ba1\u4e13\u4e1a\u5230\u5e95\u662f\u51fa\u4e8e\u4ec0\u4e48\u76ee\u7684\u3002\n\n\u6bd4\u5982\u8bf4\u5b66\u4e2d\u6587\u7684\u4f1a\u8bf4\u4ece\u5c0f\u559c\u6b22\u770b\u4e66\uff0c\u4f1a\u6709\u4ece\u5c0f\u559c\u6b22\u4f1a\u8ba1\u501f\u554a\u8d37\u554a\u7684\u7684\u4eba\u5417\uff1f'</span>],</div><div class="line">     <span class="string">'name'</span>: [],</div><div class="line">     <span class="string">'title'</span>: [<span class="string">u'\n\n'</span>, <span class="string">u'\n\n'</span>],</div><div class="line">     <span class="string">'url'</span>: <span class="string">'http://www.zhihu.com/question/20688855/answer/15861368'</span>&#125;</div><div class="line">...</div></pre></td></tr></table></figure>
<p>#6. 存在问题</p>
<ul>
<li><code>Rule</code>设计不能实现全网站抓取, 只是设置了简单的问题的抓取</li>
<li><code>Xpath</code>设置不严谨, 需要重新思考</li>
<li><code>Unicode</code>编码应该转换成<code>UTF-8</code></li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2014/12/22/Django搭建简易博客教程-一-Django简介/" class="prev">PREV</a><a href="/2014/12/19/IOS之UITabBarController/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'snow-memory';
var disqus_identifier = '2014/12/19/Python爬虫-八-Scrapy模拟登录/';
var disqus_title = 'Python爬虫(八)--Scrapy模拟登录';
var disqus_url = 'http://andrewliu.in/2014/12/19/Python爬虫-八-Scrapy模拟登录/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//snow-memory.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2017 <a href="http://andrewliu.in">Andrew Liu</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-58158116-2",'auto');ga('send','pageview');</script></body></html>