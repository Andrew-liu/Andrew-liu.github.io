<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Logistic回归和随机梯度下降算法 · Snow Memory | Andrew Liu</title><meta name="description" content="Logistic回归和随机梯度下降算法 - Andrew Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://andrewliu.in/atom.xml" title="Snow Memory | Andrew Liu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/dinosaurliu" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Andrew-liu" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Logistic回归和随机梯度下降算法</h1><div class="post-info">Oct 15, 2014</div><div class="post-content"><p>#0. logistic回归概述</p>
<ul>
<li>优点：计算代价不高，易于理解和实现。</li>
<li>缺点：容易欠拟合，分类精度可能不高。</li>
<li>适用数据类型：数值型和标称型数据。</li>
<li>类别：分类算法。</li>
<li>试用场景：解决二分类问题。</li>
<li>简述：Logistic回归算法基于Sigmoid函数，或者说Sigmoid就是逻辑回归函数。Sigmoid函数定义如下：1/（1+exp（-z))。函数值域范围(0,1)。可以用来做分类器。</li>
</ul>
<a id="more"></a>
<p>#1. 回归和Logistic回归的概念<br>回归<br>:   假设有一些数据点, 我们利用一条直线对这些点进行拟合(<code>这条线成为最佳拟合直线</code>), 这个拟合的过程称为回归</p>
<p>Logistic回归主要思想<br>:   根据现有数据对分类边界线建立回归公式, 以此进行分类</p>
<blockquote>
<p>   Logistic regression是线性回归的一种，线性回归是一种回归</p>
</blockquote>
<p> 回归其实就是对已知公式的未知参数进行估计。比如已知公式是y = a*x + b，未知参数是a和b。我们现在有很多真实的(x,y)数据（训练样本），回归就是利用这些数据对a和b的取值去自动估计。估计的方法是在给定训练样本点和已知的公式后，对于一个或多个未知参数，机器会自动枚举参数的所有可能取值，直到找到那个最符合样本点分布的参数（或参数组合）。（实际运算有一些优化算法，如果<code>随机梯度下降</code>,不会去枚举的）.</p>
<p>##1.1. Sigmoid函数<br>在logistic回归中需要用到一个函数的性质:<br>sigmoid的函数有一个性质是讲结果映射到概率（[0,1]）之间进行输出</p>
<p><img src="http://hiphotos.baidu.com/hehehehello/pic/item/b81c5cb56260e19137d3ca76.jpg" alt="Logistic方程"></p>
<p><img src="http://hiphotos.baidu.com/hehehehello/pic/item/70c8710982bc58f02fddd476.jpg" alt="Logistic方程"></p>
<p><code>Logistic Regression 就是一个被sigmoid方程归一化后的线性回归</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def sigmoid(vecX) :</div><div class="line">    return 1.0 / (1 + math.exp(-vecX))</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Logistic的一般过程</div><div class="line">1. 收集数据 : 采用任意方法收集数据</div><div class="line">2. 准备数据 : 由于需要进行距离计算, 因此要求数据类型为数值型. 另外, 结构化数据格式则最佳</div><div class="line">3. 分析数据 : 采用任意方法对数据进行分析</div><div class="line">4. 训练算法 : 大部分时间用于训练, 训练目的是为了找到最佳的分类回归系数</div><div class="line">5. 测试算法 : 分类测试</div><div class="line">6. 使用算法 : 输入一些数据, 转换成对应的结构化数值, 然后, 基于训练好的回归系数对这些数值进行简单的回归计算, 判定他们属于那个类别</div></pre></td></tr></table></figure>
<p>#2. 梯度下降方法</p>
<blockquote>
<p>梯度下降为了找到Logistic回归分类器在数据集中的最佳回归系数,也是整个logistic回归中最重要的一部训练系数</p>
</blockquote>
<p>梯度其实就是高数求导方法，对E这个公式针对每个维数（w0，w1）求偏导后的向量▽E(w)=（∂E/∂w0,∂E/∂w1）<br>对E这个公式针对每个维数（w0，w1）求偏导后的向量▽E(w)=（∂E/∂w0,∂E/∂w1）<br>梯度为最陡峭上升的方向，对应的梯度下降的训练法则为： w=w - alpha <em> ▽E(w)     这里的η代表学习速率，决定梯度下降搜索中的步长 。<br>上式的w是向量，即可用将该式写成分量形式为:wi=wi- alpha </em>∂E/∂wi<br>现在关键就使计算∂E/∂wi：<br>推导过程很简单，书上写的很详细，这里只记录结论(其实就是对目标函数求导):<br>∂E/∂wi=∑（h(x)-y）*(xi)<br>这里的∑是对样本空间，即训练集进行一次遍历，耗费时间较大，可以使用梯度下降的随机近似.</p>
<p>##2.1. 随机梯度下降的随机近似<br>既然是随机近似，肯定是用近似方法来改善梯度下降时候的时间复杂度问题。<br>在∂E/∂wi=∑（h(x)-y）<em>(xi) 的时候∑耗费了大量的时间，特别是在训练集庞大的时候。<br>如果把求和去掉如何，即变为∂E/∂wi=（h(x)-y）</em>(xi)会减少很多时间消耗,就形成了随机梯度下降算法</p>
<p><code>只是要注意一下标准的梯度下降和随机梯度下降的区别</code>：</p>
<ol>
<li>标准下降时在权值更新前汇总所有样例得到的标准梯度，随机下降则是通过考察每次训练实例来更新。</li>
<li>对于步长 alpha的取值，标准梯度下降的alpha比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降使用的是近似的梯度，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。</li>
<li>当E（w）有多个局部极小值时，随机梯度反而更可能避免进入局部极小值中。</li>
</ol>
<p>##2.2. 随机梯度下降的伪码和实现<br>随机梯度下降在我的理解就是用来训练回归系数值!</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">伪代码：</div><div class="line"></div><div class="line">初始化回归系数为1</div><div class="line">重复下面步骤直到收敛&#123;</div><div class="line">对数据集中每个样本</div><div class="line">计算该样本的梯度</div><div class="line">使用alpha x gradient来更新回归系数</div><div class="line">&#125;</div><div class="line">返回回归系数值</div></pre></td></tr></table></figure>
<p>对logistic Regression来说，梯度下降算法新鲜出炉，如下<br><img src="http://yspe2371e4aa7697989.yunshipei.cn/dHlwZT1mdyZzaXplPTY0MCZzcmM9YUhSMGNDVXpRU1V5UmlVeVJtbHRaeTVpYkc5bkxtTnpaRzR1Ym1WMEpUSkdNakF4TkRBek1qZ3dNRFF6TWprME1qRWxNMFozWVhSbGNtMWhjbXNsTWtZeUpUSkdkR1Y0ZENVeVJtRklVakJqUkc5MlRESktjMkl5WTNWWk0wNXJZbWsxZFZwWVVYWmxiVGt4WlVocmQwOVJKVE5FSlRORUpUSkdabTl1ZENVeVJqVmhOa3cxVERKVUpUSkdabTl1ZEhOcGVtVWxNa1kwTURBbE1rWm1hV3hzSlRKR1NUQktRbEZyUmtOTlFTVXpSQ1V6UkNVeVJtUnBjM052YkhabEpUSkdOekFsTWtabmNtRjJhWFI1SlRKR1UyOTFkR2hGWVhOMA==" alt="随机梯度下降"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">def stocGradDescent(self) :</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        计算权重系数向量</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        m, n = weiCount(self.train_vec_list)</div><div class="line">        for j in range(10):</div><div class="line">            dataIndex = range(m)#生成一个长度为1500左右的list</div><div class="line">            for index in range(m) :</div><div class="line">                alpha = 4 / (1.0 + j + index) + 0.01</div><div class="line">                randIndex = int(random.uniform(0, len(dataIndex)))</div><div class="line">                h = sigmoid(sumArray(self.train_vec_list[randIndex], self.weights))</div><div class="line">                #print &quot;h:&quot;,h,</div><div class="line">                error = self.class_list[randIndex] - h</div><div class="line">                #print &quot;error&quot;, error,</div><div class="line">                self.weights = arraySub(self.weights, arrayMulti(alpha, error, self.train_vec_list[randIndex]))</div><div class="line">                del(dataIndex[randIndex])</div><div class="line">                </div><div class="line">def weiCount(data_mat) :</div><div class="line">#返回train_vec_list行数和列数</div><div class="line">    return len(data_mat), len(data_mat[0])</div><div class="line"></div><div class="line">def sumArray(lineVec, weights) :</div><div class="line">    #两向量的内积</div><div class="line">    total = 0</div><div class="line">    for index in range(len(lineVec)) :</div><div class="line">        total += (lineVec[index] * weights[index])</div><div class="line">    #print &quot;total:&quot;, total,</div><div class="line">    return total</div><div class="line"></div><div class="line">def arrayMulti(count, error, lineVec) :</div><div class="line">    for index in range(len(lineVec)) :</div><div class="line">        lineVec[index] = count * error * lineVec[index]</div><div class="line">    return  lineVec</div><div class="line"></div><div class="line">def arraySub(weights, lineVec) :</div><div class="line">    for index in range(len(weights)) :</div><div class="line">        weights[index] = weights[index] + lineVec[index]</div><div class="line">    return weights</div></pre></td></tr></table></figure>
<p>#3. 特征词和特征向量的选取<br>对文章分类时, 需要将文章抽象成机器可以识别的一种向量或者其他实现发现, 我在编写程序时使用了提取文章特征词的形式.</p>
<p>##3.1. 特征词提取来源和方法</p>
<ol>
<li>对于每一个类别的文章都由属于整个特别的词语, 称为<code>特征词</code>,通过特征词来标识文章的类别</li>
<li>特征词的提取我知道两种方案</li>
</ol>
<blockquote>
<p>在取特征词的时候一定要注意去停用词, <code>停用词</code>会在后文讲到</p>
</blockquote>
<ul>
<li>使用TF也就是词频来统计特征, 对整个类别的文章统计所有词语出现的频率, 然后区别类别中出现频率最高的K个词语作为特征词(<code>本人的程序使用了这种方法</code>)</li>
<li>使用TF-IDF来提取特征词, 这里给出维基百科的链接<br><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CB0QFjAA&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTf%25E2%2580%2593idf&amp;ei=ffk8VPn9FoLe7AbuuYHwAw&amp;usg=AFQjCNGBK9lcvu3prTxLLIy_acVG-IZFXQ&amp;sig2=Vhnz2IoJ4gRoTOM2nblphQ&amp;bvm=bv.77412846,d.bGQ&amp;cad=rjt" target="_blank" rel="external">TF-IDF</a><br><a href="http://coolshell.cn/articles/8422.html" target="_blank" rel="external">TF-IDF详细讲解</a><br><a href="http://blog.csdn.net/haoni123321/article/details/30081633" target="_blank" rel="external">文本向量表示及TF-IDF词汇权值</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">#TF-IDF的实现</div><div class="line">def ComputeFreq(wordlist, text):</div><div class="line">    result = []</div><div class="line">    for word in wordlist:</div><div class="line">        countword = text.count(word)</div><div class="line">        texted = nltk.word_tokenize(text)</div><div class="line">        length = len(texted)</div><div class="line">        freq = countword/length</div><div class="line">        temp = &#123;&#125;</div><div class="line">        temp[&apos;word&apos;] = word</div><div class="line">        temp[&apos;freq&apos;] = freq</div><div class="line">        #print freq</div><div class="line">        result.append(temp)</div><div class="line">    return result</div><div class="line"></div><div class="line">def Computetfidf(wordfreq, corpus):</div><div class="line">    result = []</div><div class="line">    for item in wordfreq:</div><div class="line">        word = item[&apos;word&apos;]</div><div class="line">        tf = item[&apos;freq&apos;]</div><div class="line">        dlength = len(corpus)</div><div class="line">        count = 1</div><div class="line">        for line in corpus:</div><div class="line">            if line.find(word)!=-1:</div><div class="line">                count = count+1</div><div class="line">        idf = math.log10(dlength/count)</div><div class="line">        tfidf = tf * idf</div><div class="line">        temp = &#123;&#125;</div><div class="line">        temp[&apos;word&apos;] = word</div><div class="line">        temp[&apos;tfidf&apos;] = tfidf</div><div class="line">        result.append(temp)</div><div class="line">    result.sort(lambda x,y : -cmp(x[&apos;tfidf&apos;], y[&apos;tfidf&apos;]))  </div><div class="line">    return result</div></pre></td></tr></table></figure>
<p>##3.2. 停用词<br>在计算词频或者IDF时, 会发生一种现象, 一些并没有特色的词语出现的次数反而最高, 例如: <code>的, 和, 我等词语</code>,这些词语被称为<code>停用词</code>, 所以我们应该在分词后将他们去掉</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def makeStopWord(self) :</div><div class="line">     with open(&quot;stop_word.txt&quot;, &quot;r&quot;) as stop_file :</div><div class="line">         self.stop_word = stop_file.read()</div><div class="line">     self.stop_word = self.stop_word.split()</div></pre></td></tr></table></figure>
<p>##3.3. 特征词选取的具体实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def readEssay(self):</div><div class="line">        &quot;&quot;&quot;生成特征词集合&quot;&quot;&quot;</div><div class="line">        one_block = &quot;&quot;</div><div class="line">        with open(&quot;./lily/&quot; + dict_list[0] +&quot;.txt&quot;, &quot;r&quot;) as my_file :</div><div class="line">            line = my_file.read()</div><div class="line">        word_list = list(jieba.cut(line, cut_all = False))</div><div class="line">        word_dict = &#123;&#125;</div><div class="line">        for word in word_list :</div><div class="line">            if word not in word_dict :</div><div class="line">                word_dict[word] = 1</div><div class="line">            else :</div><div class="line">                word_dict[word] += 1</div><div class="line">        word_dict = sorted(word_dict.iteritems(), key = lambda d :d[1], reverse = True)</div><div class="line">        self.feature = []</div><div class="line">        for word, fre in word_dict :</div><div class="line">            self.feature.append(word)</div><div class="line">        self.feature = self.feature[ : self.dimension]</div><div class="line">        #for word in self.feature :</div><div class="line">        #    print word,</div></pre></td></tr></table></figure>
<p>##3.4. 特征向量的形成<br>当一个位置类别的文章进入分类器的时候, 我们首先应该讲这个文章转化成机器可以识别的一种语言, 即形成<strong>特征向量</strong><br>步骤:</p>
<ol>
<li>将文章分词,并且去停用词</li>
<li>将文章分词与特征词比对, 出现在特征词中的词语, 对应的向量位置为1, 特征向量的长度应该等于特征词的个数, <code>例如 : 将1000个特征词, 文章某个词语出现特征词集合中, 这个特征词在特征词集合的位置为123, 则生成的特征向量的第123位置1</code></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">def makeVector(self, line) :</div><div class="line">    &quot;&quot;&quot;将一个帖子的关键词转化成对应的特征向量&quot;&quot;&quot;</div><div class="line">    vector = [0] * self.dimension</div><div class="line">    final = []</div><div class="line">    #去停用词</div><div class="line">    for word  in line :</div><div class="line">        #word = word.encode(&apos;utf8&apos;)</div><div class="line">        if word not in self.stop_word :</div><div class="line">            final.append(word)</div><div class="line">    #生成特征向量</div><div class="line">    for word in final :</div><div class="line">        if word in self.feature : </div><div class="line">            vector[self.feature.index(word)] += 1</div><div class="line">    #print vector</div><div class="line">    return vector</div></pre></td></tr></table></figure>
<p>#4. 文章分类<br>将一篇文章生成特征向量, 然后用特征向量与系数向量做内积(<code>系数向量就是随机梯度向量的训练结果</code>), 然后使用sigmoid函数进行转化</p>
<ol>
<li>概率大于0.5的说明属于分类1</li>
<li>概率小于0.5的说明属于分类0</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def classifyVector(self, vecX) :</div><div class="line">    prob = sigmoid(sumArray(vecX, self.weights))</div><div class="line">    return prob</div></pre></td></tr></table></figure>
<p>#5. 多重分类</p>
<ol>
<li>由于logistic回归只能处理二重分类, 也就是回答是属于一个类别还是不是属于这个类别的问题. </li>
<li>当出现多重分类时, 需要进行一些思想的转化</li>
<li>将多个分类取其中一个作为正确分类, 将剩余所有类别作为错误分类(<code>one vs rest</code>), 进行训练系数</li>
<li>然后使用多个分类器, 实现多重分类</li>
</ol>
<p>更多具体信息请看<a href="http://en.wikipedia.org/wiki/Multiclass_classification" target="_blank" rel="external">one vs rest</a></p>
<blockquote>
<p>这种分类方式会产生一些偏差, 例如: 样本不均匀</p>
</blockquote>
<p>#6. 总结</p>
<blockquote>
<p>Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，参数的求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法有可以简化为随机梯度上升算法。</p>
</blockquote>
<p>更多logistic回归的资料 :<br><a href="http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf" target="_blank" rel="external">logistic回归</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2014/10/16/2014-10-16-Rio_Design_Implement/" class="prev">PREV</a><a href="/2014/10/10/2014-10-10-BootStrap学习笔记续/" class="next">NEXT</a></div><div class="copyright"><p>© 2014 - 2017 <a href="http://andrewliu.in">Andrew Liu</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-58158116-2",'auto');ga('send','pageview');</script></body></html>