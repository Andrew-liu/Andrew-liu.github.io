<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> K最邻近算法和局部敏感哈希LSH · Snow Memory | Andrew Liu</title><meta name="description" content="K最邻近算法和局部敏感哈希LSH - Andrew Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://andrewliu.in/atom.xml" title="Snow Memory | Andrew Liu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/dinosaurliu" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Andrew-liu" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">K最邻近算法和局部敏感哈希LSH</h1><div class="post-info">Oct 23, 2014</div><div class="post-content"><p>#1. K最近邻(k-Nearest Neighbor)分类算法</p>
<p>采用测量不同特征值之间的距离方法进行分类。<br>K近邻分类算法的主要思想：<code>如果一个样本在特征空间中的k个最相似）的样本中的大多数属于某一个类别，则该样本也属于这个类别</code>(这里对于最相似的判定主要是通过特征值向量的距离)</p>
<p>##1.1. 算法特点及伪代码</p>
<ul>
<li>KNN算法中，所选择的邻居都是已经正确分类的对象(训练集)</li>
<li>KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。</li>
<li>当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数</li>
</ul>
<a id="more"></a>
<p>伪代码描述</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">1. 计算已知类别数据集中的点与当前点之间的距离</div><div class="line">2. 按照距离递增次序排序</div><div class="line">3. 选取与当前点距离最小的k个点</div><div class="line">4. 确定前k个点所在类别的出现频率</div><div class="line">5.返回前k个点出现频率最高的类别作为当前点的预测分类</div></pre></td></tr></table></figure>
<p>##1.2. Python实现</p>
<blockquote>
<p>这篇博文写的有些匆忙, 如果以后有时间的话, 我会进行重新整理</p>
</blockquote>
<ul>
<li>特征抽取,  对于每个类别的文本进行特征抽取,  获取特征词集合, 用于匹配测试文本, 生成特征向量</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def extract_feature() :</div><div class="line">    text = &quot;&quot;</div><div class="line">    post_set = []; class_set = []; feature = []</div><div class="line">    jieba.analyse.set_stop_words(&quot;stop_word.txt&quot;)</div><div class="line">    for index in range(len(dict_list)) :</div><div class="line">        with open(&quot;./lily/&quot; + dict_list[index] + &quot;.txt&quot;, &quot;r&quot;) as my_file :</div><div class="line">            #读入每个板块所有的帖子            </div><div class="line">            for post in my_file : </div><div class="line">                post_set.append(list(jieba.cut(post, cut_all = False))) #将post字符串存入list</div><div class="line">                class_set.append(index)</div><div class="line">                text += post</div><div class="line">            feature.extend(jieba.analyse.extract_tags(text, 100))</div><div class="line">    return feature, post_set, class_set</div></pre></td></tr></table></figure>
<ul>
<li>将每一个训练文本生成对应的特征向量, 首先对训练文本分词, 每一个文本表示成一个词list, 然后遍历整个词list, 与特征词集合做匹配, 匹配成功, 则当前为计算词频, 匹配不成功置0</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def createQuery(post_set, feature) :</div><div class="line">    query_set = []</div><div class="line">    vec_size = len(feature)</div><div class="line">    for post in post_set :</div><div class="line">        vector = [0] * vec_size</div><div class="line">        for word in post :</div><div class="line">            if word in feature :</div><div class="line">                vector[feature.index(word)] += 1</div><div class="line">        query_set.append(vector)</div><div class="line">    return query_set</div><div class="line">def makeVector(post, feature, size) :</div><div class="line">    vector = [0] * size</div><div class="line">    for word in post :</div><div class="line">        if word in feature :</div><div class="line">            vector[feature.index(word)] += 1</div><div class="line">    return vector</div></pre></td></tr></table></figure>
<ul>
<li>计算欧式距离, 获得不同向量之间的距离</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">def knnClassify(vecX, query_set, class_set, k) :</div><div class="line">    query_set = array(query_set)</div><div class="line">    query_set_size = query_set.shape[0]</div><div class="line">    mid_mat = tile(vecX, (query_set_size, 1)) - query_set</div><div class="line">    sqrt_two = mid_mat ** 2</div><div class="line">    sqrt_distances = sqrt_two.sum(axis = 1)</div><div class="line">    distances = sqrt_distances ** 0.5</div><div class="line">    sorted_dist = distances.argsort()</div><div class="line">    class_count = &#123;&#125;</div><div class="line">    for i in range(k) :</div><div class="line">        label = class_set[sorted_dist[i]]</div><div class="line">        class_count[label] = class_count.get(label, 0) + 1</div><div class="line">    sort_class_count = sorted(class_count.iteritems(), key = operator.itemgetter(1), reverse = True)</div><div class="line">    return sort_class_count[0][0]</div></pre></td></tr></table></figure>
<ul>
<li>对测试文本进行暴力搜索, 找到距离文本最近的k个文本, 然后找到k文本中出现次数最后的类别, 整个类别就是测试文本的类别</li>
</ul>
<blockquote>
<p>距离搜索的缺陷: 当训练文本过大时, 需要用测试文本与每个训练文本计算欧式距离, 导致计算时间过长,</p>
</blockquote>
<p>#2.局部敏感哈希</p>
<blockquote>
<p>LSH的基本思想是：将原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小</p>
</blockquote>
<p>#2.1. LSH的具体描述<br>如果我们对原始数据进行一些hash映射后，我们希望原先相邻的两个数据能够被hash到相同的桶内，具有相同的桶号。对原始数据集合中所有的数据都进行hash映射后，我们就得到了一个hash table，这些原始数据集被分散到了hash table的桶内，每个桶会落入一些原始数据，属于同一个桶内的数据就有很大可能是相邻的，当然也存在不相邻的数据被hash到了同一个桶内。因此，如果我们能够找到这样一些hash functions，使得经过它们的哈希映射变换后，原始空间中相邻的数据落入相同的桶内的话，那么我们在该数据集合中进行近邻查找就变得容易了，我们只需要将查询数据进行哈希映射得到其桶号，然后取出该桶号对应桶内的所有数据，再进行线性匹配即可查找到与查询数据相邻的数据。<code>LSH将一个在超大集合内查找相邻元素的问题转化为了在一个很小的集合内查找相邻元素的问题，显然计算量下降了很多</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">hash function需要满足以下两个条件：</div><div class="line">1）如果d(x,y) ≤ d1， 则h(x) = h(y)的概率至少为p1；</div><div class="line">2）如果d(x,y) ≥ d2， 则h(x) = h(y)的概率至多为p2；</div><div class="line">其中d(x,y)表示x和y之间的距离，d1 &lt; d2， h(x)和h(y)分别表示对x和y进行hash变换。</div><div class="line">满足以上两个条件的hash functions称为(d1,d2,p1,p2)-sensitive。而通过一个或多个(d1,d2,p1,p2)-sensitive的hash function对原始数据集合进行hashing生成一个或多个hash table的过程称为Locality-sensitive Hashing。</div></pre></td></tr></table></figure>
<p>通俗来讲，就是距离较近的点映射到同一个位置的概率大，距离较远的点映射到同一个位置的概率小。这对下面将点集映射得到候选结果集的操作很重要，能保证得到的候选结果集是可用的。</p>
<p>LSH算法可以看做两步: </p>
<ol>
<li>将与测试文本完全不相关的向量剔除掉(<code>哈希函数映射过程</code>)，只保留与测试文本相似的概率较大的向量作为待比较向量(<code>哈希表中希望产生冲突, 使类似的文本能够映射到同一个桶里</code>)</li>
<li>讲测试文本向量与剩余向量逐一对比, 找到前k个临近的向量</li>
</ol>
<p>#2.Python实现</p>
<p>使用 Cosine distance </p>
<blockquote>
<p>Cosine distance：cos(theta) = A·B / |A||B|</p>
</blockquote>
<p>常用来判断两个向量之间的夹角，夹角越小，表示它们越相似</p>
<ul>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">//Cosine distance的计算</div><div class="line">#计算向量的模</div><div class="line">def magnitudeProd(vec1, vec2) :</div><div class="line">    total1 = 0</div><div class="line">    total2 = 0</div><div class="line">    for i in xrange(dimension) :</div><div class="line">        val1 = vec1[i]</div><div class="line">        val2 = vec2[i]</div><div class="line">        total1 += val1**2</div><div class="line">        total2 += val2**2</div><div class="line">    return (total1 * total2)**0.5</div><div class="line"></div><div class="line">#计算两个向量之间的夹角, 看是否在一个角度范围内</div><div class="line">def cosineDistance(vec1, vec2) :</div><div class="line">    mag_prod = magnitudeProd(vec1, vec2)</div><div class="line">    if mag_prod == 0 :</div><div class="line">        return 0</div><div class="line">    return dotProduct(vec1, vec2) / magnitudeProd(vec1, vec2)</div><div class="line"></div><div class="line">#计算两个向量的内积</div><div class="line">def dotProduct(vec1, vec2) :</div><div class="line">    total = 0</div><div class="line">    for i in xrange(len(vec1)) :</div><div class="line">        total += vec1[i] * vec2[i]</div><div class="line">    return total</div></pre></td></tr></table></figure>
</li>
<li><p>计算测试文本和所有训练文本的哈希值生成哈希表</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def localitySensitiveHash(vec, planes) :</div><div class="line">    dot_prod_list = [dotProduct(vec, plane) for plane in planes]</div><div class="line">    return (sum([2 ** i if dot_prod_list[i] &gt; 0 else 0 for i in xrange(0, len(dot_prod_list))]) % 8)</div><div class="line"></div><div class="line">hash_table = [(localitySensitiveHash(row, planes), class_set[class_index]) for class_index, row in enumerate(train_set)]</div><div class="line">hash_dict = defaultdict(list)</div><div class="line">for (hash_value, class_index) in hash_table :</div><div class="line">    hash_dict[hash_value].append(class_index)</div></pre></td></tr></table></figure>
<ul>
<li>最后对一个桶内的所有的向量进行KNN算法计算距离, 找出前K个出现频率最高的</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2014/10/26/2014-10-26-Python正则表达式/" class="prev">PREV</a><a href="/2014/10/23/2014-10-23-Assistance/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'snow-memory';
var disqus_identifier = '2014/10/23/2014-10-23-K最邻近算法和局部敏感哈希LSH/';
var disqus_title = 'K最邻近算法和局部敏感哈希LSH';
var disqus_url = 'http://andrewliu.in/2014/10/23/2014-10-23-K最邻近算法和局部敏感哈希LSH/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//snow-memory.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2017 <a href="http://andrewliu.in">Andrew Liu</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-58158116-2",'auto');ga('send','pageview');</script></body></html>