<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> MapReduce之WordCount · Snow Memory | Andrew Liu</title><meta name="description" content="MapReduce之WordCount - Andrew Liu"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://andrewliu.in/atom.xml" title="Snow Memory | Andrew Liu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/dinosaurliu" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/Andrew-liu" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">MapReduce之WordCount</h1><div class="post-info">Mar 29, 2015</div><div class="post-content"><p>本博客采用创作共用版权协议, 要求署名、非商业用途和保持一致. 转载本博客文章必须也遵循<a href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh" target="_blank" rel="external">署名-非商业用途-保持一致</a>的创作共用协议.</p>
<h2 id="1-再述MapReduce计算模型"><a href="#1-再述MapReduce计算模型" class="headerlink" title="#1. 再述MapReduce计算模型"></a>#1. 再述MapReduce计算模型</h2><ul>
<li>JobTracker用于管理和调度工作(<code>一个集群只有一个JobTracker</code>)</li>
<li>TaskTracker用于执行工作</li>
<li>每个MapReduce任务被初始化为一个Job, 每个Job分为Map(接收键值对)和Reduce阶段</li>
</ul>
<a id="more"></a>
<p>InputSplit(存储分片长度和记录数据位置的数组)把输入数据传送给单独的Map, 数据传给Map后, Map将输入分片传送到InputFormat()上, InputFormat()(<code>用来生成可供Map处理的键值对</code>)调用getRecordReader()方法生成RecordReader, RecordReader再通过createKey(), createValue()方法创建可供Map处理的键值对.<code>TextInputFormat</code>是Hadoop默认的输入方法, 每个文件都读作为Map的输入, 每行数组生成一条键值对(key在数据分片中的字节偏移量LongWritable, value是每行内容Text)</p>
<h2 id="2-编译打包运行WordCount"><a href="#2-编译打包运行WordCount" class="headerlink" title="#2. 编译打包运行WordCount"></a>#2. 编译打包运行WordCount</h2><p>总结一下通过Eclipse来编译打包运行自己写的MapReduce程序(<code>基于Hadoop2.6.0</code>)</p>
<p>##2.1. Hadoop库</p>
<p>在编写Hadoop程序会用到Hadoop库, 所以需要一些Hadoop库文件, 用于编译</p>
<ul>
<li>hadoop-common-2.6.0.jar</li>
<li>hadoop-mapreduce-client-core-2.6.0.jar</li>
<li>hadoop-test-1.2.1.jar</li>
</ul>
<p>下载地址<a href="http://mvnrepository.com/artifact/org.apache.hadoop" target="_blank" rel="external">Group: org.apache.hadoop</a>下载对应版本的库文件</p>
<p>##2.2. 创建工程</p>
<ol>
<li>使用Eclipse创建名为WordCount的工程</li>
<li>在<code>Project Properties -&gt; Java Build Path -&gt; Libraries -&gt; Add External Jars</code> 添加第一步所下载Jar包, 点击OK</li>
<li>创建WordCount.java源文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">#WorkCount.java</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line">import java.util.StringTokenizer;</div><div class="line"></div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class WordCount &#123;</div><div class="line"></div><div class="line">  public static class TokenizerMapper</div><div class="line">       extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123;</div><div class="line"></div><div class="line">    private final static IntWritable one = new IntWritable(1);</div><div class="line">    private Text word = new Text();</div><div class="line">    /*</div><div class="line">     * LongWritable 为输入的key的类型</div><div class="line">     * Text 为输入value的类型</div><div class="line">     * Text-IntWritable 为输出key-value键值对的类型</div><div class="line">     */</div><div class="line">    public void map(Object key, Text value, Context context</div><div class="line">                    ) throws IOException, InterruptedException &#123;</div><div class="line">      StringTokenizer itr = new StringTokenizer(value.toString());  // 将TextInputFormat生成的键值对转换成字符串类型</div><div class="line">      while (itr.hasMoreTokens()) &#123;</div><div class="line">        word.set(itr.nextToken());</div><div class="line">        context.write(word, one);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  public static class IntSumReducer</div><div class="line">       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</div><div class="line">    private IntWritable result = new IntWritable();</div><div class="line">    /*</div><div class="line">     * Text-IntWritable 来自map的输入key-value键值对的类型</div><div class="line">     * Text-IntWritable 输出key-value 单词-词频键值对</div><div class="line">     */</div><div class="line">    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,</div><div class="line">                       Context context</div><div class="line">                       ) throws IOException, InterruptedException &#123;</div><div class="line">      int sum = 0;</div><div class="line">      for (IntWritable val : values) &#123;</div><div class="line">        sum += val.get();</div><div class="line">      &#125;</div><div class="line">      result.set(sum);</div><div class="line">      context.write(key, result);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  public static void main(String[] args) throws Exception &#123;</div><div class="line">    Configuration conf = new Configuration();  // job的配置</div><div class="line">    Job job = Job.getInstance(conf, &quot;word count&quot;);  // 初始化Job</div><div class="line">    job.setJarByClass(WordCount.class);</div><div class="line">    job.setMapperClass(TokenizerMapper.class);</div><div class="line">    job.setCombinerClass(IntSumReducer.class);</div><div class="line">    job.setReducerClass(IntSumReducer.class);  </div><div class="line">    job.setOutputKeyClass(Text.class);</div><div class="line">    job.setOutputValueClass(IntWritable.class);</div><div class="line">    FileInputFormat.addInputPath(job, new Path(args[0]));  // 设置输入路径</div><div class="line">    FileOutputFormat.setOutputPath(job, new Path(args[1]));  // 设置输出路径</div><div class="line">    System.exit(job.waitForCompletion(true) ? 0 : 1);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>##2.3. 打包源文件</p>
<ol>
<li>在<code>File -&gt; Export -&gt; Java -&gt; JAR File</code>, 然后点击next</li>
<li>选中WordCount源文件, 设置输出路径和文件名WordCount.jar, 选择Finish则打包成功</li>
<li>在输出路径生成Wordcount.jar</li>
</ol>
<p>##2.4. 启动HDFS服务</p>
<p>打开目录/usr/local/Cellar/hadoop/2.6.0/sbin</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ start-dfs.sh  #启动HDFS</div><div class="line">$ jps  #验证是否启动成功</div><div class="line">8324 Jps</div><div class="line">8069 DataNode</div><div class="line">7078 NodeManager</div><div class="line">6696 NameNode</div><div class="line">6987 ResourceManager</div><div class="line">3453 SecondaryNameNode</div><div class="line">$ stop-dfs.sh  #停止HDFS</div></pre></td></tr></table></figure>
<p>成功启动服务后, 可以直接在浏览器中输入<a href="http://localhost:50070/" target="_blank" rel="external">http://localhost:50070/</a>访问Hadoop页面</p>
<p>##2.5. 将输入文件上传到HDFS</p>
<p>打开目录/usr/local/Cellar/hadoop/2.6.0/bin</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#在HDFS上创建输入/输出文件夹</div><div class="line">$ hdfs dfs -mkdir /user</div><div class="line">$ hdfs dfs -mkdir /user/input</div><div class="line">$ hdfs dfs -ls /user</div><div class="line">#上传本地file中文件到集群的input目录下</div><div class="line">$ hdfs dfs -put /Users/andrew_liu/Java/Hadoop/input/* /user/input</div><div class="line">#查看上传到HDFS输入文件夹中到文件</div><div class="line">$ hdfs dfs -ls /user/input</div><div class="line"></div><div class="line">#输出结果</div><div class="line">-rw-r--r--   1 andrew_liu supergroup    1808033 2015-04-05 12:37 /user/input/rural.txt</div><div class="line">-rw-r--r--   1 andrew_liu supergroup    2246756 2015-04-05 12:37 /user/input/science.txt</div></pre></td></tr></table></figure>
<p>##2.6. 运行Jar文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#在当前文件夹创建一个工作目录</div><div class="line">$ mkdir WorkSpace </div><div class="line">#将打包号的Jar包复制到当前工作目录</div><div class="line">$cp /usr/local/Cellar/hadoop/2.6.0/bin/WorkSpace/WordCount.jar ./WorkSpace</div><div class="line">#运行Jar文件, 各字段的意义(Hadoop打包命令, 指定Jar文件, 指定Jar文件入口类, 指定job的HDFS上的输入文件目录, 指定job的HDFS输出文件目录)</div><div class="line">$ hadoop jar WorkSpace/WordCount.jar WordCount /user/input output</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">... 省略部分</div><div class="line">    File System Counters</div><div class="line">        FILE: Number of bytes read=2025025</div><div class="line">        FILE: Number of bytes written=4443318</div><div class="line">        FILE: Number of read operations=0</div><div class="line">        FILE: Number of large read operations=0</div><div class="line">        FILE: Number of write operations=0</div><div class="line">        HDFS: Number of bytes read=10356334</div><div class="line">        HDFS: Number of bytes written=616286</div><div class="line">        HDFS: Number of read operations=25</div><div class="line">        HDFS: Number of large read operations=0</div><div class="line">        HDFS: Number of write operations=5</div><div class="line">    Map-Reduce Framework</div><div class="line">        Map input records=33907</div><div class="line">        Map output records=663964</div><div class="line">        Map output bytes=6687108</div><div class="line">        Map output materialized bytes=1005779</div><div class="line">        Input split bytes=216</div><div class="line">        Combine input records=663964</div><div class="line">        Combine output records=68147</div><div class="line">        Reduce input groups=55800</div><div class="line">        Reduce shuffle bytes=1005779</div><div class="line">        Reduce input records=68147</div><div class="line">        Reduce output records=55800</div><div class="line">        Spilled Records=136294</div><div class="line">        Shuffled Maps =2</div><div class="line">        Failed Shuffles=0</div><div class="line">        Merged Map outputs=2</div><div class="line">        GC time elapsed (ms)=187</div><div class="line">        Total committed heap usage (bytes)=1323827200</div><div class="line">    Shuffle Errors</div><div class="line">        BAD_ID=0</div><div class="line">        CONNECTION=0</div><div class="line">        IO_ERROR=0</div><div class="line">        WRONG_LENGTH=0</div><div class="line">        WRONG_MAP=0</div><div class="line">        WRONG_REDUCE=0</div><div class="line">    File Input Format Counters</div><div class="line">        Bytes Read=4054789</div><div class="line">    File Output Format Counters</div><div class="line">        Bytes Written=616286</div></pre></td></tr></table></figure>
<p>##2.7. 查看运行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#查看FS上output目录内容</div><div class="line">$ hdfs dfs -ls output</div><div class="line">-rw-r--r--   1 andrew_liu supergroup          0 2015-04-05 13:20 output/_SUCCESS</div><div class="line">-rw-r--r--   1 andrew_liu supergroup     616286 2015-04-05 13:20 output/part-r-00000 # 存放结果文件</div><div class="line">#查看结果输出文件内容</div><div class="line">hdfs dfs -cat output/part-r-00000</div></pre></td></tr></table></figure>
<p>##2.8. MapReduce运行流程</p>
<ul>
<li>JobTracker调度任务个TaskTracker, TaskTracker执行任务时, 返回进度报告, 如果执行失败, JobTracker将任务分配给另一个TaskTracker, 知道任务完成</li>
<li>数据按照TextInputFormat被处理成InputSplit, 输入到Map中, Map读取InputSplit指定位置的数据, <code>按照设定的方式处理数据</code>, 最后写入本地磁盘</li>
<li>Reduce读取Map输出数据, 合并value, 然后输出到HDFS上</li>
</ul>
<h2 id="3-MapReduce任务优化"><a href="#3-MapReduce任务优化" class="headerlink" title="#3. MapReduce任务优化"></a>#3. MapReduce任务优化</h2><ul>
<li>计算性能优化</li>
<li>I/O操作优化</li>
</ul>
<ol>
<li>任务调度(就近原则, 选用空闲原则)</li>
<li>数据预处理应合理设置block快大小及Map和Reduce任务数量</li>
<li>combine函数用于本地合并数据的函数, 运行用户combine用于本地合并, 可减少网络I/O的消耗</li>
<li>对Map输出和最终结果压缩</li>
<li>自定义comparator实现数据的二进制比较, 省去数据序列化和反序列化时间</li>
</ol>
<h2 id="4-Hadoop流"><a href="#4-Hadoop流" class="headerlink" title="#4. Hadoop流"></a>#4. Hadoop流</h2><p>当一个可执行未见作为Mapper时, 每个Map任务以一个独立的进程启动可执行未见, 任务执行时, 会把输入划分成行提供给可执行文件, 并作为Map的标准输入, Map从标准输出中收集数据, 并转换为<code>&lt;key, value&gt;</code>输出</p>
<p>Reduce任务启动可执行文件, 将键值对转化为标准输入, Reduce从标准输出中收集数据, 并转换为<code>&lt;key, value&gt;</code>输出</p>
<h2 id="5-参考链接"><a href="#5-参考链接" class="headerlink" title="#5. 参考链接"></a>#5. 参考链接</h2><ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">MapReduce Tutorial 2.6.0</a></li>
<li><a href="http://mvnrepository.com/artifact/org.apache.hadoop" target="_blank" rel="external">Group: org.apache.hadoop</a></li>
<li><a href="http://glebche.appspot.com/static/hadoop-ecosystem/mapreduce-job-java.html" target="_blank" rel="external">HADOOP TUTORIAL: CREATING MAPREDUCE JOBS IN JAVA</a></li>
<li><a href="http://glebche.appspot.com/static/hadoop-ecosystem/hadoop-hive-tutorial.html" target="_blank" rel="external">BIG DATA AND HADOOP</a></li>
<li><code>&lt;Hadoop Action&gt;</code></li>
<li><a href="http://blog.csdn.net/andie_guo/article/details/44055863" target="_blank" rel="external">【Hadoop基础教程】5、Hadoop之单词计数</a></li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2015/04/01/算法设计与分析/" class="prev">PREV</a><a href="/2015/03/22/NumPy基础/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'snow-memory';
var disqus_identifier = '2015/03/29/MapReduce之WordCount/';
var disqus_title = 'MapReduce之WordCount';
var disqus_url = 'http://andrewliu.in/2015/03/29/MapReduce之WordCount/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//snow-memory.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2017 <a href="http://andrewliu.in">Andrew Liu</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-58158116-2",'auto');ga('send','pageview');</script></body></html>